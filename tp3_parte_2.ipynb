{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2QsDT3UtstgOisuSK4A3D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nacho2904/orga_de_datos/blob/main/tp3_parte_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TP3 Parte II: Baseline"
      ],
      "metadata": {
        "id": "JMf_D_txvJfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Del análisis exploratorio de la parte I hemos aprendido algunas cosas:\n",
        "- Hay algunas columnas que no nos aportan información. En particular *did* tiene utilidad por no estar en la mayoría de columnas, y *s-label* no sabemos cómo interpretarlo. *Language* tampoco aporta demasiado debido a que falta en muchas canciones \n",
        "\n",
        "- Algunas columnas requieren preprocessing. Las tres columnas de texto que tenemos, *track-name*, *lyrics* y *artist*, no pueden ser usadas directamente. *mode* y *key*, por otro lado, son features categóricas. *a_genres* también es una variable categórica que contiene varias clases. Luego hay que preprocesar las features de texto para crear nuevos features útiles, y preprocesar las features categóricas para poder utilizarlas en la regresión logística.\n",
        "\n",
        "- Noo tenemos suficientes observaciones para algunos de los posibles valores del target."
      ],
      "metadata": {
        "id": "FBkL9n3Vt7j-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import preprocessing\n",
        "from google.colab import drive \n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "path_a_training_set = 'gdrive/MyDrive/TP3 dataset music/train.parquet'\n",
        "\n",
        "df_music_train = pd.read_parquet(path_a_training_set)"
      ],
      "metadata": {
        "id": "CfvksKXqhTcq",
        "outputId": "b3a9a752-185d-438b-b77b-c18e4ca58b89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_music_train_filtered = df_music_train.drop(labels=[\"s-label\", \"did\"], axis=1)\n",
        "df_music_train_filtered.info()"
      ],
      "metadata": {
        "id": "-BCToRkEi8Db",
        "outputId": "2d65bdaf-6362-41b5-d9be-a9e334cc82e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 31383 entries, 0 to 34336\n",
            "Data columns (total 22 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   track_name        31383 non-null  object \n",
            " 1   lyric             31380 non-null  object \n",
            " 2   genre             31383 non-null  object \n",
            " 3   language          24021 non-null  object \n",
            " 4   popularity        31383 non-null  int64  \n",
            " 5   artist            31383 non-null  object \n",
            " 6   a_genres          31383 non-null  object \n",
            " 7   a_songs           31383 non-null  float64\n",
            " 8   a_popularity      31383 non-null  float64\n",
            " 9   acousticness      31383 non-null  float64\n",
            " 10  danceability      31383 non-null  float64\n",
            " 11  duration_ms       31383 non-null  int64  \n",
            " 12  energy            31383 non-null  float64\n",
            " 13  instrumentalness  31383 non-null  float64\n",
            " 14  key               31383 non-null  object \n",
            " 15  liveness          31383 non-null  float64\n",
            " 16  loudness          31383 non-null  float64\n",
            " 17  mode              31383 non-null  object \n",
            " 18  speechiness       31383 non-null  float64\n",
            " 19  tempo             31383 non-null  float64\n",
            " 20  time_signature    31383 non-null  object \n",
            " 21  valence           31383 non-null  float64\n",
            "dtypes: float64(11), int64(2), object(9)\n",
            "memory usage: 5.5+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En primer lugar notamos que las variables categóricas *key* y *time-signature* ambas son ordinales. La primera representa el tono dominante en la canción., y tomaremos el orden dado en [este blog](https://viva.pressbooks.pub/openmusictheory/chapter/pitch-and-pitch-class/). El *time-signature* es una medida de la cantidad de pulsos por unidad, y también está ordenado naturalmente.*"
      ],
      "metadata": {
        "id": "hFJV8C5gjLEu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QZ8RerM8kM3U",
        "outputId": "307fda93-e388-441f-b95a-3e80cf1fb01b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['1/4', '3/4', '4/4', '5/4'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ordinalEncoder = preprocessing.OrdinalEncoder(categories = [['C', ''],['1/4', '3/4', '4/4', '5/4']])\n",
        "ordinalEncoder.fit(df_music_train_filtered[[\"time_signature\"]])"
      ],
      "metadata": {
        "id": "dQohiJ4Lhn86",
        "outputId": "e99e7792-4184-4b2f-f6f7-af0dee97c920",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrdinalEncoder(categories=[['1/4', '3/4', '4/4', '5/4']])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ordinalEncoder.transform(df_music_train_filtered[\"time_signature\"].to_frame())"
      ],
      "metadata": {
        "id": "2Ksk2-Fpksl5",
        "outputId": "0adda9ce-793c-48c2-a2e3-3edb3eb01999",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.],\n",
              "       [2.],\n",
              "       [2.],\n",
              "       ...,\n",
              "       [1.],\n",
              "       [2.],\n",
              "       [2.]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}