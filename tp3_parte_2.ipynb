{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOD0TgwuYPZUDbuIAHLmXNR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nacho2904/orga_de_datos/blob/main/tp3_parte_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TP3 Parte II: Baseline"
      ],
      "metadata": {
        "id": "JMf_D_txvJfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Del análisis exploratorio de la parte I hemos aprendido algunas cosas:\n",
        "- Hay algunas columnas que no nos aportan información. En particular *did* tiene utilidad por no estar en la mayoría de columnas. El *artist_name* no debería proporcionar demasiada información al modelo teniendo en cuenta que tenemos su número de canciones y sus géneros predilectos.\n",
        "\n",
        "- Algunas columnas requieren preprocessing. Las tres columnas de texto que tenemos, *track-name*, *lyrics* y *artist*, no pueden ser usadas directamente. *mode* y *key*, por otro lado, son features categóricas. *a_genres* también es una variable categórica que contiene varias clases. Luego hay que preprocesar las features de texto para crear nuevos features útiles, y preprocesar las features categóricas para poder utilizarlas en la regresión logística.\n",
        "\n",
        "- No tenemos suficientes observaciones para algunos de los posibles valores del target."
      ],
      "metadata": {
        "id": "FBkL9n3Vt7j-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import preprocessing\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from google.colab import drive \n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "import functools\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "path_a_training_set = 'gdrive/MyDrive/TP3 dataset music/train.parquet'\n",
        "path_a_test_set = 'gdrive/MyDrive/TP3 dataset music/test.parquet'\n",
        "\n",
        "\n",
        "df_music_train = pd.read_parquet(path_a_training_set)\n",
        "df_music_test = pd.read_parquet(path_a_test_set)"
      ],
      "metadata": {
        "id": "CfvksKXqhTcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad7bd368-b80b-43a9-9ec1-7ed868a022db"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_music_train_filtered = df_music_train.drop(labels=[\"did\"], axis=1)\n",
        "df_music_train_filtered.info()"
      ],
      "metadata": {
        "id": "-BCToRkEi8Db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81d2fd5d-7ade-4b45-8b9b-60f0d1c97674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 31383 entries, 0 to 34336\n",
            "Data columns (total 23 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   track_name        31383 non-null  object \n",
            " 1   lyric             31380 non-null  object \n",
            " 2   genre             31383 non-null  object \n",
            " 3   language          24021 non-null  object \n",
            " 4   popularity        31383 non-null  int64  \n",
            " 5   artist            31383 non-null  object \n",
            " 6   a_genres          31383 non-null  object \n",
            " 7   a_songs           31383 non-null  float64\n",
            " 8   a_popularity      31383 non-null  float64\n",
            " 9   s-label           7004 non-null   float64\n",
            " 10  acousticness      31383 non-null  float64\n",
            " 11  danceability      31383 non-null  float64\n",
            " 12  duration_ms       31383 non-null  int64  \n",
            " 13  energy            31383 non-null  float64\n",
            " 14  instrumentalness  31383 non-null  float64\n",
            " 15  key               31383 non-null  object \n",
            " 16  liveness          31383 non-null  float64\n",
            " 17  loudness          31383 non-null  float64\n",
            " 18  mode              31383 non-null  object \n",
            " 19  speechiness       31383 non-null  float64\n",
            " 20  tempo             31383 non-null  float64\n",
            " 21  time_signature    31383 non-null  object \n",
            " 22  valence           31383 non-null  float64\n",
            "dtypes: float64(12), int64(2), object(9)\n",
            "memory usage: 5.7+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "fwptY_P6y2pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En primer lugar notamos que las variables categóricas *key* y *time-signature* ambas son ordinales. La primera representa el tono dominante en la canción., y tomaremos el orden dado en [este blog](https://viva.pressbooks.pub/openmusictheory/chapter/pitch-and-pitch-class/). El *time-signature* es una medida de la cantidad de pulsos por unidad, y también está ordenado naturalmente. *Mode* es una variable binaria así que la encodeamos como 0 y 1."
      ],
      "metadata": {
        "id": "hFJV8C5gjLEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ordinalEncoder = preprocessing.OrdinalEncoder(categories = [[\"Minor\", \"Major\"],['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B'],\n",
        "                                                            ['1/4', '3/4', '4/4', '5/4']])"
      ],
      "metadata": {
        "id": "dQohiJ4Lhn86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para las letras de los se me ocurre aplicar algo de NLP para realizar sentiment analysis."
      ],
      "metadata": {
        "id": "FA6kTDsNyR4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('vader_lexicon')\n",
        "def apply_sentiment_analysis_to_lyrics(df_music: pd.DataFrame) -> pd.DataFrame:\n",
        "  sia = SentimentIntensityAnalyzer()\n",
        "  negative, neutral, positive, compound = 0, 1, 2, 3\n",
        "  sentimentAnalysisOfLyrics = df_music[\"lyric\"].map(lambda lyric: list(sia.polarity_scores(lyric).values()))\n",
        "  negativeScoreOfLyrics = sentimentAnalysisOfLyrics.map(lambda row: row[negative])\n",
        "  positiveScoreOfLyrics = sentimentAnalysisOfLyrics.map(lambda row: row[positive])\n",
        "  neutralScoreOfLyrics = sentimentAnalysisOfLyrics.map(lambda row: row[neutral])\n",
        "  compoundScoreOfLyrics = sentimentAnalysisOfLyrics.map(lambda row: row[compound])\n",
        "  return pd.DataFrame(pd.concat([negativeScoreOfLyrics, positiveScoreOfLyrics,neutralScoreOfLyrics,compoundScoreOfLyrics], axis = 1))"
      ],
      "metadata": {
        "id": "hMMWFU6D4BON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbddbb95-bf02-4cfc-ac22-cf228b850416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otra cosa que se me ocurre es usar la suma del tf-idf para medir qué tan \"rico\" es el vocabulario de una canción: La idea es que canciones de géneros como el pop que son más masivos y apuntan a un público general tendrán una suma de tf-idf alto, mientras que géneros como la música alternativa deberían usar un vocabulario más \"peculiar\"."
      ],
      "metadata": {
        "id": "TYwPrgmfJZos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "def get_sum_tfidf_from_lyrics(df_music: pd.DataFrame) -> pd.DataFrame:\n",
        "  stopwords = set(nltk.corpus.stopwords.words(\"english\")).union(set(nltk.corpus.stopwords.words(\"spanish\"))).union(set(nltk.corpus.stopwords.words(\"french\")))\n",
        "  vectorizer = TfidfVectorizer(input = \"content\", stop_words = stopwords)\n",
        "  vectorizer.fit(df_music[\"lyric\"])\n",
        "  return pd.DataFrame([np.sum(tfidf_vector) for tfidf_vector in vectorizer.transform(df_music[\"lyric\"])])"
      ],
      "metadata": {
        "id": "seazRU21KuOZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c3ce41c-20da-453c-fff9-478e570f32a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eliminate_genres_without_enough_observations(df_music: pd.DataFrame) -> pd.DataFrame:\n",
        "  df_music_recuento_filas_por_genero = df_music.groupby(\"genre\").count().reset_index()[[\"genre\", \"track_name\"]].rename(\n",
        "    columns = {\"track_name\": \"rowCount\"}).sort_values(\"rowCount\")\n",
        "  problematic_genres = list(df_music_recuento_filas_por_genero[df_music_recuento_filas_por_genero[\"rowCount\"] < 50].genre)[1:]\n",
        "  return df_music[~df_music[\"genre\"].isin(problematic_genres)]"
      ],
      "metadata": {
        "id": "TQCnc16lQAxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos conseguir varias features de las longitudes de los textos"
      ],
      "metadata": {
        "id": "10nohwQQNmOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "def get_length_transforms_for_text(df_music: pd.DataFrame) -> pd.DataFrame:\n",
        "  df_music[\"number_of_lines\"] = df_music[\"lyric\"].map(lambda lyric: len(lyric.split(\"\\n\")))\n",
        "  df_music[\"number_of_tokens\"] = df_music[\"lyric\"].map(lambda lyric: len(nltk.word_tokenize(lyric)))\n",
        "  df_music[\"length_lyrics\"] = df_music[\"lyric\"].map(lambda lyric: len(lyric))\n",
        "  df_music[\"length_of_track_name\"] = df_music[\"track_name\"].map(lambda track_name: len(track_name))\n",
        "  df_music[\"tokens_of_track_name\"] = df_music[\"track_name\"].map(lambda track_name: len(nltk.word_tokenize(track_name)))\n",
        "  return df_music[[\"length_lyrics\", \"length_of_track_name\", \"number_of_lines\", \"number_of_tokens\"]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1QqQUHIEHzj",
        "outputId": "9cb93a45-e95c-4e40-e2f4-0864ee15da4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analizo las lyrics también usando [POS](https://www.guru99.com/pos-tagging-chunking-nltk.html)"
      ],
      "metadata": {
        "id": "Kl3xUtILNqi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.pos_tag(nltk.word_tokenize(\"Hey how are you\"))\n",
        "useful_pos_tags = {\"FW\", \"JJR\", \"JJS\", \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"PRP\", \"RB\", \"RBR\", \"RBS\",\n",
        "                   \"UH\", \"VB\", \"VBG\", \"VBD\", \"VBN\", \"VBP\", \"VBZ\"}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeUMKy6VRbbn",
        "outputId": "2bdd2950-cff5-48fa-e46e-35b5bab4eefd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_POS_vector_from_lyric_POS(lyric_POS):\n",
        "  useful_pos_tags = [\"FW\", \"JJR\", \"JJS\", \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"PRP\", \"RB\", \"RBR\", \"RBS\",\n",
        "                   \"UH\", \"VB\", \"VBG\", \"VBD\", \"VBN\", \"VBP\", \"VBZ\"]\n",
        "  POS_dictionary = {pos_tag:0 for pos_tag in useful_pos_tags}\n",
        "  POS_dictionary.update({\"other\":0})\n",
        "\n",
        "  for token in lyric_POS:\n",
        "    if token[1] in useful_pos_tags:\n",
        "      POS_dictionary[token[1]] += 1\n",
        "    else:\n",
        "      POS_dictionary[\"other\"] += 1\n",
        "\n",
        "  return [POS_dictionary[pos_tag] for pos_tag in POS_dictionary]\n",
        "  \n",
        "\n",
        "def get_POS_chunk_taggin_counts_for_text(df_music: pd.DataFrame) -> pd.DataFrame:\n",
        "  useful_pos_tags = {\"FW\", \"JJR\", \"JJS\", \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"PRP\", \"RB\", \"RBR\", \"RBS\",\n",
        "                   \"UH\", \"VB\", \"VBG\", \"VBD\", \"VBN\", \"VBP\", \"VBZ\"}\n",
        "  df_music[\"lyric_POS\"] = df_music[\"lyric\"].map(lambda lyric: nltk.pos_tag(nltk.word_tokenize(lyric)))\n",
        "  df_music[\"lyric_POS\"] = df_music[\"lyric_POS\"].map(lambda lyric_pos: get_POS_vector_from_lyric_POS(lyric_pos))\n",
        "  return pd.DataFrame(df_music[\"lyric_POS\"].to_list())\n"
      ],
      "metadata": {
        "id": "LOEyM2E-Ny6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separamos un test de validation seleccionando diferentes artistas"
      ],
      "metadata": {
        "id": "A269jvKR12DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_music_filtered_genres = eliminate_genres_without_enough_observations(df_music_train_filtered)\n",
        "artists = list(df_music_filtered_genres[\"artist\"].unique())\n",
        "train_artists = set(artists[:int(0.8*len(artists))])\n",
        "validation_artists = set(artists[int(0.8*len(artists)):])\n",
        "train_set = df_music_filtered_genres[df_music_filtered_genres[\"artist\"].isin(train_artists)]\n",
        "validation_set = df_music_filtered_genres[df_music_filtered_genres[\"artist\"].isin(validation_artists)]"
      ],
      "metadata": {
        "id": "0WXfDLhA174X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "text_features = [\"track_name\", \"lyric\", \"artist\"]\n",
        "\n",
        "numerical_features = [\"a_songs\", \"a_popularity\", \"popularity\", \"acousticness\", \"danceability\", \"duration_ms\",\n",
        "                   \"energy\", \"instrumentalness\", \"liveness\", \"loudness\", \"speechiness\", \"tempo\", \"valence\"]\n",
        "\n",
        "ordinal_features = [\"mode\", \"key\", \"time_signature\"]\n",
        "\n",
        "one_hot_features = [\"language\"]\n",
        "\n",
        "artist_genres = [\"a_genres\", \"genre\"]\n",
        "\n",
        "label = [\"genre\"]\n",
        "\n",
        "identity_transformer = preprocessing.FunctionTransformer(None)\n",
        "\n",
        "full_processor = ColumnTransformer(transformers=[\n",
        "    ('text_sentiment_analysis', preprocessing.FunctionTransformer(apply_sentiment_analysis_to_lyrics), text_features),\n",
        "    ('text_tf_idf', preprocessing.FunctionTransformer(get_sum_tfidf_from_lyrics), text_features),\n",
        "    ('text_simple_transforms', preprocessing.FunctionTransformer(get_length_transforms_for_text), text_features),\n",
        "    ('text_POS_count', preprocessing.FunctionTransformer(get_POS_chunk_taggin_counts_for_text), text_features),\n",
        "    ('one-hot', preprocessing.OneHotEncoder(), one_hot_features),\n",
        "    ('numerical', identity_transformer, numerical_features),\n",
        "    ('ordinal', ordinalEncoder, ordinal_features),\n",
        "])\n",
        "\n",
        "logistic_regression_pipeline = Pipeline(steps = [\n",
        "    ('preprocess_X', full_processor),\n",
        "    ('model', LogisticRegression(penalty='l2', C = 1, solver = \"liblinear\", max_iter = 500, fit_intercept = True))\n",
        "])\n"
      ],
      "metadata": {
        "id": "ApBgPVOuocwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regresión Logística"
      ],
      "metadata": {
        "id": "7qByPTkay5qJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labelEncoder = preprocessing.LabelEncoder()\n",
        "labelEncoder.fit(eliminate_genres_without_enough_observations(df_music_train_filtered)[\"genre\"])\n",
        "y_train = labelEncoder.transform(train_set[\"genre\"])\n",
        "X_train = full_processor.fit_transform(train_set.fillna(\"\"))\n",
        "#logistic_regression_pipeline.fit(train_set.fillna(\"\"), y_train)"
      ],
      "metadata": {
        "id": "PEUXTnO9hY41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_valid = labelEncoder.transform(validation_set[\"genre\"])\n",
        "X_validation = full_processor.transform(validation_set.fillna(\"\"))"
      ],
      "metadata": {
        "id": "pAXQW8M-i-ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import top_k_accuracy_score\n",
        "model = LogisticRegression(penalty='l2', C = 1, solver = \"liblinear\", max_iter = 500, fit_intercept = True)\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict_proba(X_validation)\n",
        "top_k_accuracy_score(y_valid, preds, k=2, labels=model.classes_)"
      ],
      "metadata": {
        "id": "lRv8CYgBkoHl",
        "outputId": "beaa5c1c-b5bb-41e7-84cc-1fb704745da4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.33361884368308353"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import loguniform\n",
        "\n",
        "space = dict()\n",
        "space['solver'] = ['liblinear']\n",
        "space['penalty'] = ['l1', 'l2']\n",
        "#space['C'] = loguniform(1e-5, 100)              \n",
        "space['C'] = np.arange(0,0.2,0.01)\n",
        "logistic_regression_search = GridSearchCV(LogisticRegression(max_iter = 200), param_grid = space, scoring='accuracy', cv=2)\n",
        "\n",
        "\n",
        "logistic_regression_with_found_hyperparameters = logistic_regression_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6VOuIYQ7M-m",
        "outputId": "679fb75a-92a1-4fd8-c70d-08b890ee0051"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "4 fits failed out of a total of 80.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "4 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1541, in fit\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\", line 1198, in _fit_liblinear\n",
            "    sample_weight,\n",
            "  File \"sklearn/svm/_liblinear.pyx\", line 52, in sklearn.svm._liblinear.train_wrap\n",
            "ValueError: b'C <= 0'\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.26765728 0.21821247 0.28459059 0.22340543\n",
            " 0.29565359 0.22062094 0.30013139 0.22242693 0.30472223 0.21896484\n",
            " 0.31006559 0.21704614 0.31074291 0.2176104  0.30927528 0.22916229\n",
            " 0.31138247 0.22310434 0.31047935 0.21990584 0.31352743 0.22528685\n",
            " 0.31194688 0.22114759 0.31149528 0.22261512 0.31333918 0.22212586\n",
            " 0.31059215 0.22084661 0.31345199 0.22126061 0.31318862 0.21787391\n",
            " 0.31458086 0.22016898 0.31360253 0.22509874]\n",
            "  category=UserWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_with_found_hyperparameters.best_params_"
      ],
      "metadata": {
        "id": "18TvW99tdYvh",
        "outputId": "e9aff197-b945-41e8-9955-b6eb9c1d4f2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 0.18, 'penalty': 'l1', 'solver': 'liblinear'}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = logistic_regression_with_found_hyperparameters.predict_proba(X_validation)\n",
        "top_k_accuracy_score(y_valid, preds, k=2, labels=model.classes_)"
      ],
      "metadata": {
        "id": "QpxPY5y1qR7r",
        "outputId": "11c9671d-e654-4ddc-e550-41df8b096a48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4661670235546039"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicción en el test set"
      ],
      "metadata": {
        "id": "LyXzF4cLqzEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = full_processor.transform(eliminate_genres_without_enough_observations(df_music_test.fillna(\"\")))\n",
        "y_test = labelEncoder.transform(eliminate_genres_without_enough_observations(df_music_test)[\"genre\"])"
      ],
      "metadata": {
        "id": "yDaTf940q2ag"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = logistic_regression_with_found_hyperparameters.predict_proba(X_test)\n",
        "top_k_accuracy_score(y_test, preds, k=2, labels=model.classes_)"
      ],
      "metadata": {
        "id": "NHXwfI4Trm_l",
        "outputId": "9092a41c-6c12-46d5-a910-523580cc20d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5247693884523403"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(preds).to_csv(\"predictions_logistic_regression.csv\")"
      ],
      "metadata": {
        "id": "Fb4GwGFvs5N9"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¿Qué features son los más importantes para predecir con el mejor modelo?"
      ],
      "metadata": {
        "id": "MZhRzgcdzCUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fitteamos un decision tree al dataset para analizar las features de mayor importancia"
      ],
      "metadata": {
        "id": "rsO4Dfq_sAFz"
      }
    }
  ]
}