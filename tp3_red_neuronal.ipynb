{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCHkldgBNcarUsyKiu9KSk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nacho2904/orga_de_datos/blob/main/tp3_red_neuronal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h_aJs-4Im0Ta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c566f85-ada8-46a1-fe57-bb0393023979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import top_k_accuracy_score\n",
        "from google.colab import drive \n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import functools\n",
        "drive.mount('/content/gdrive')\n",
        "path_a_training_set = 'gdrive/MyDrive/TP3 dataset music/train.parquet'\n",
        "path_a_test_set = 'gdrive/MyDrive/TP3 dataset music/test.parquet'\n",
        "\n",
        "\n",
        "df_music_train = pd.read_parquet(path_a_training_set).fillna(\"\")\n",
        "df_music_test = pd.read_parquet(path_a_test_set).fillna(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "fwptY_P6y2pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('vader_lexicon')\n",
        "def apply_sentiment_analysis_to_lyrics(df_music: pd.DataFrame) -> pd.DataFrame:\n",
        "  sia = SentimentIntensityAnalyzer()\n",
        "  negative, neutral, positive, compound = 0, 1, 2, 3\n",
        "  sentimentAnalysisOfLyrics = df_music[\"lyric\"].map(lambda lyric: list(sia.polarity_scores(lyric).values()))\n",
        "  negativeScoreOfLyrics = sentimentAnalysisOfLyrics.map(lambda row: row[negative])\n",
        "  positiveScoreOfLyrics = sentimentAnalysisOfLyrics.map(lambda row: row[positive])\n",
        "  neutralScoreOfLyrics = sentimentAnalysisOfLyrics.map(lambda row: row[neutral])\n",
        "  compoundScoreOfLyrics = sentimentAnalysisOfLyrics.map(lambda row: row[compound])\n",
        "  return pd.DataFrame(pd.concat([negativeScoreOfLyrics, positiveScoreOfLyrics,neutralScoreOfLyrics,compoundScoreOfLyrics], axis = 1))"
      ],
      "metadata": {
        "id": "hMMWFU6D4BON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c225e55-8562-4f3e-8f94-668f4ebf2426"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "genres = list(df_music_train[\"genre\"].unique())\n",
        "def get_vectorizers_by_genre(df_music: pd.DataFrame) -> dict:\n",
        "  df_music_lyric_tokenized = df_music.copy().fillna(\"\")\n",
        "  df_music_lyric_tokenized[\"lyric\"] = df_music_lyric_tokenized[\"lyric\"].map(lambda lyric: set(nltk.word_tokenize(lyric)))\n",
        "  df_music_grouped_by_genre = df_music_lyric_tokenized[[\"genre\", \"lyric\"]].groupby('genre').agg(lambda x: functools.reduce(set.union, x)).reset_index()\n",
        "  vocabs = dict(zip(df_music_grouped_by_genre.genre.to_list(), df_music_grouped_by_genre.lyric.to_list()))\n",
        "  stopwords = set(nltk.corpus.stopwords.words(\"english\")).union(set(nltk.corpus.stopwords.words(\"spanish\"))).union(set(nltk.corpus.stopwords.words(\"french\")))\n",
        "  vectorizers = {genre: TfidfVectorizer(input = \"content\", stop_words = stopwords, vocabulary = vocabs[genre]) for genre in genres}\n",
        "  for genre in genres:\n",
        "    vectorizers[genre].fit(df_music[df_music[\"genre\"] == genre][\"genre\"])\n",
        "  return vectorizers\n",
        "\n",
        "vectorizers = get_vectorizers_by_genre(df_music_train)"
      ],
      "metadata": {
        "id": "UuLe4UY82Fv9",
        "outputId": "89ccc5de-3a06-4d86-c31c-2531be6ab400",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:1323: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  \"Upper case characters found in\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sum_tfidf_from_lyrics(df_music: pd.DataFrame) -> pd.DataFrame:\n",
        "  df_music = df_music.fillna(\"\")\n",
        "  column_names = [\"sum_tfidf_for_\" + genre.lower() for genre in genres]\n",
        "  for i in range(0, len(genres)):\n",
        "    df_music[column_names[i]] = np.sum(vectorizers[genres[i]].transform(df_music[\"lyric\"]), axis = 1)\n",
        "  return df_music[column_names]"
      ],
      "metadata": {
        "id": "Hn7ojEFq-YXi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eliminate_genres_without_enough_observations(df_music: pd.DataFrame) -> pd.DataFrame:\n",
        "  df_music_recuento_filas_por_genero = df_music.groupby(\"genre\").count().reset_index()[[\"genre\", \"track_name\"]].rename(\n",
        "    columns = {\"track_name\": \"rowCount\"}).sort_values(\"rowCount\")\n",
        "  problematic_genres = list(df_music_recuento_filas_por_genero[df_music_recuento_filas_por_genero[\"rowCount\"] < 50].genre)[1:]\n",
        "  return df_music[~df_music[\"genre\"].isin(problematic_genres)]"
      ],
      "metadata": {
        "id": "TQCnc16lQAxu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "def get_length_transforms_for_text(df_music: pd.DataFrame) -> pd.DataFrame:\n",
        "  df_music[\"number_of_lines\"] = df_music[\"lyric\"].map(lambda lyric: len(lyric.split(\"\\n\")))\n",
        "  df_music[\"number_of_tokens\"] = df_music[\"lyric\"].map(lambda lyric: len(nltk.word_tokenize(lyric)))\n",
        "  df_music[\"length_lyrics\"] = df_music[\"lyric\"].map(lambda lyric: len(lyric))\n",
        "  df_music[\"length_of_track_name\"] = df_music[\"track_name\"].map(lambda track_name: len(track_name))\n",
        "  return df_music[[\"length_lyrics\", \"length_of_track_name\", \"number_of_lines\", \"number_of_tokens\"]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1QqQUHIEHzj",
        "outputId": "15a54e1f-dfd1-4513-dc69-4620917f80bc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def get_POS_vector_from_lyric_POS(lyric_POS):\n",
        "  useful_pos_tags = [\"FW\", \"JJR\", \"NN\", \"NNS\", \"NNP\", \"PDT\", \"PRP\", \"RB\", \"RBR\",\n",
        "                   \"UH\", \"VB\", \"VBG\", \"VBD\", \"VBN\", \"VBP\", \"VBZ\"]\n",
        "  POS_dictionary = {pos_tag:0 for pos_tag in useful_pos_tags}\n",
        "  POS_dictionary.update({\"other\":0})\n",
        "\n",
        "  for token in lyric_POS:\n",
        "    if token[1] in useful_pos_tags:\n",
        "      POS_dictionary[token[1]] += 1\n",
        "    else:\n",
        "      POS_dictionary[\"other\"] += 1\n",
        "\n",
        "  return [POS_dictionary[pos_tag] for pos_tag in POS_dictionary]\n",
        "  \n",
        "\n",
        "def get_POS_chunk_taggin_counts_for_text(df_music: pd.DataFrame) -> pd.DataFrame:\n",
        "  df_music[\"lyric_POS\"] = df_music[\"lyric\"].map(lambda lyric: nltk.pos_tag(nltk.word_tokenize(lyric)))\n",
        "  df_music[\"lyric_POS\"] = df_music[\"lyric_POS\"].map(lambda lyric_pos: get_POS_vector_from_lyric_POS(lyric_pos))\n",
        "  return pd.DataFrame(df_music[\"lyric_POS\"].to_list())\n"
      ],
      "metadata": {
        "id": "LOEyM2E-Ny6P",
        "outputId": "939bbe6a-26d3-4d40-8c89-eb85334fb6a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_music_filtered_genres = eliminate_genres_without_enough_observations(df_music_train)\n",
        "artists = list(df_music_filtered_genres[\"artist\"].unique())\n",
        "train_artists = set(artists[:int(0.9*len(artists))])\n",
        "validation_artists = set(artists[int(0.9*len(artists)):])\n",
        "train_set = df_music_filtered_genres[df_music_filtered_genres[\"artist\"].isin(train_artists)]\n",
        "validation_set = df_music_filtered_genres[df_music_filtered_genres[\"artist\"].isin(validation_artists)]"
      ],
      "metadata": {
        "id": "0WXfDLhA174X"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_hot_encoder(df_music: pd.DataFrame, df_training: pd.DataFrame) -> pd.DataFrame:\n",
        "  df_training_grouped_by_lang = df_training.groupby(\"language\").mean().reset_index()[[\"language\", \"popularity\", \"a_popularity\", \"loudness\"]]\n",
        "  df_new_columns = df_music.merge(df_training_grouped_by_lang, on = \"language\", how = \"left\")\n",
        "  return df_new_columns[[\"popularity_y\", \"a_popularity_y\", \"loudness_y\"]].fillna(0)\n",
        "\n",
        "mean_hot_encoder_using_training_set = lambda df_to_encode: mean_hot_encoder(df_to_encode, df_music_train) "
      ],
      "metadata": {
        "id": "wG1TiABKECHD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "text_features = [\"track_name\", \"lyric\", \"artist\"]\n",
        "\n",
        "numerical_features = [\"a_songs\", \"a_popularity\", \"popularity\", \"acousticness\", \"danceability\", \"duration_ms\",\n",
        "                   \"energy\", \"instrumentalness\", \"liveness\", \"loudness\", \"speechiness\", \"tempo\", \"valence\"]\n",
        "\n",
        "ordinal_features = [\"key\", \"time_signature\"]\n",
        "\n",
        "one_hot_features = [\"mode\"]\n",
        "\n",
        "mean_enc_features = [\"language\"]\n",
        "\n",
        "artist_genres = [\"a_genres\", \"genre\"]\n",
        "\n",
        "label = [\"genre\"]\n",
        "\n",
        "identity_transformer = preprocessing.FunctionTransformer(None)\n",
        "\n",
        "full_processor = ColumnTransformer(transformers=[\n",
        "    ('text_sentiment_analysis', preprocessing.FunctionTransformer(apply_sentiment_analysis_to_lyrics), text_features),\n",
        "    ('text_tf_idf', preprocessing.FunctionTransformer(get_sum_tfidf_from_lyrics), text_features),\n",
        "    ('text_simple_transforms', preprocessing.FunctionTransformer(get_length_transforms_for_text), text_features),\n",
        "    ('text_POS_count', preprocessing.FunctionTransformer(get_POS_chunk_taggin_counts_for_text), text_features),\n",
        "    ('mean_encoding', preprocessing.FunctionTransformer(mean_hot_encoder_using_training_set), list(df_music_train.columns)),\n",
        "    ('one_hot_encoding', preprocessing.OneHotEncoder(), one_hot_features),\n",
        "    ('numerical', identity_transformer, numerical_features),\n",
        "    ('ordinal', preprocessing.OrdinalEncoder(categories = [['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B'],\n",
        "                                                            ['1/4', '3/4', '4/4', '5/4']]), ordinal_features)])\n"
      ],
      "metadata": {
        "id": "ApBgPVOuocwU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Red Neuronal"
      ],
      "metadata": {
        "id": "h4qA0iu40-GY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = full_processor.fit_transform(train_set)"
      ],
      "metadata": {
        "id": "2BSfUO_v1aEl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = preprocessing.LabelEncoder()\n",
        "label_encoder.fit(train_set.genre)\n",
        "y_train = label_encoder.transform(train_set.genre)\n",
        "X_validation = full_processor.transform(validation_set)\n",
        "y_validation = label_encoder.transform(validation_set.genre)"
      ],
      "metadata": {
        "id": "k58Z638pgVV2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_weights(model):\n",
        "    for l in model.layers:\n",
        "        if hasattr(l,\"kernel_initializer\"):\n",
        "            l.kernel.assign(l.kernel_initializer(tf.shape(l.kernel)))\n",
        "        if hasattr(l,\"bias_initializer\"):\n",
        "            l.bias.assign(l.bias_initializer(tf.shape(l.bias)))\n",
        "        if hasattr(l,\"recurrent_initializer\"):\n",
        "            l.recurrent_kernel.assign(l.recurrent_initializer(tf.shape(l.recurrent_kernel)))\n"
      ],
      "metadata": {
        "id": "of1d2SgUqZRX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_columns = X_train.shape[1]\n",
        "num_classes = len(label_encoder.classes_)\n",
        "width = 30\n",
        "depth = 1\n",
        "activation = \"ReLU\"\n",
        "\n",
        "input = tf.keras.layers.Input(shape = (num_columns))\n",
        "normalize = tf.keras.layers.Normalization()(input)\n",
        "\n",
        "hidden_layers = [tf.keras.layers.Dense(width- int(0.3*i), activation = activation, kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                                       kernel_constraint=tf.keras.constraints.MaxNorm(5))\n",
        "                  for i in range(0,depth)]\n",
        "\n",
        "for i in range(0, depth):\n",
        "  if i==0:\n",
        "    hidden_layers[i] = hidden_layers[i](input)\n",
        "  else:\n",
        "    hidden_layers[i] = hidden_layers[i](hidden_layers[i-1])\n",
        "\n",
        "\n",
        "output = tf.keras.layers.Dense(units = num_classes, activation = \"softmax\",\n",
        "                               kernel_regularizer=tf.keras.regularizers.L1(0.001),\n",
        "                               bias_regularizer=tf.keras.regularizers.L1(0.001))(hidden_layers[-1])\n",
        "model_NN = tf.keras.models.Model(inputs = input, outputs = output)\n",
        "model_NN.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yL7Q5xQougL",
        "outputId": "018f0a20-3588-4100-e535-2773494a002b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 71)]              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 30)                2160      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 21)                651       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,811\n",
            "Trainable params: 2,811\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reset_weights(model_NN)\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=150, restore_best_weights = True)\n",
        "model_NN.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate = 0.000025),loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                 metrics = [\"accuracy\"])\n",
        "\n",
        "hist = model_NN.fit(x=X_train, y=y_train, batch_size = 64, epochs=1500, callbacks = [es],\n",
        "                 validation_data=(X_validation, y_validation))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tE71MNcLqKzP",
        "outputId": "2ba77ea0-8996-48ad-b42a-09c8dc743880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1500\n",
            "453/453 [==============================] - 2s 3ms/step - loss: 72011.7109 - accuracy: 0.0058 - val_loss: 71662.8516 - val_accuracy: 4.3649e-04\n",
            "Epoch 2/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 67257.2344 - accuracy: 0.0061 - val_loss: 66774.5156 - val_accuracy: 4.3649e-04\n",
            "Epoch 3/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 62490.3711 - accuracy: 0.0088 - val_loss: 61903.4492 - val_accuracy: 0.0022\n",
            "Epoch 4/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 58105.5859 - accuracy: 0.0385 - val_loss: 57741.4141 - val_accuracy: 0.0057\n",
            "Epoch 5/1500\n",
            "453/453 [==============================] - 2s 5ms/step - loss: 54287.3008 - accuracy: 0.0417 - val_loss: 53791.6797 - val_accuracy: 0.0061\n",
            "Epoch 6/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 50528.4141 - accuracy: 0.0426 - val_loss: 49951.3047 - val_accuracy: 0.0100\n",
            "Epoch 7/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 46937.4258 - accuracy: 0.0449 - val_loss: 46124.9766 - val_accuracy: 0.0083\n",
            "Epoch 8/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 43293.9961 - accuracy: 0.0456 - val_loss: 42234.5703 - val_accuracy: 0.0122\n",
            "Epoch 9/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 39599.6875 - accuracy: 0.0470 - val_loss: 38299.8164 - val_accuracy: 0.0118\n",
            "Epoch 10/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 35865.7266 - accuracy: 0.0487 - val_loss: 34329.7266 - val_accuracy: 0.0079\n",
            "Epoch 11/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 32103.6211 - accuracy: 0.0502 - val_loss: 30331.9531 - val_accuracy: 0.0140\n",
            "Epoch 12/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 28340.2344 - accuracy: 0.0479 - val_loss: 26350.6602 - val_accuracy: 0.0140\n",
            "Epoch 13/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 24640.4023 - accuracy: 0.0457 - val_loss: 22379.1406 - val_accuracy: 0.0179\n",
            "Epoch 14/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 21042.8242 - accuracy: 0.0394 - val_loss: 18799.1855 - val_accuracy: 0.0175\n",
            "Epoch 15/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 18066.8203 - accuracy: 0.0450 - val_loss: 16023.2021 - val_accuracy: 0.0210\n",
            "Epoch 16/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 15815.9551 - accuracy: 0.0517 - val_loss: 13621.3682 - val_accuracy: 0.0223\n",
            "Epoch 17/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 13606.2871 - accuracy: 0.0519 - val_loss: 11158.4404 - val_accuracy: 0.0284\n",
            "Epoch 18/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 11344.4922 - accuracy: 0.0522 - val_loss: 8644.1367 - val_accuracy: 0.0275\n",
            "Epoch 19/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 9039.8691 - accuracy: 0.0517 - val_loss: 6090.4971 - val_accuracy: 0.0231\n",
            "Epoch 20/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 6899.3169 - accuracy: 0.0568 - val_loss: 4228.8457 - val_accuracy: 0.0423\n",
            "Epoch 21/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 5461.1777 - accuracy: 0.0576 - val_loss: 3160.1780 - val_accuracy: 0.0537\n",
            "Epoch 22/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 4389.7148 - accuracy: 0.0601 - val_loss: 2249.5063 - val_accuracy: 0.0607\n",
            "Epoch 23/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 3383.5703 - accuracy: 0.0612 - val_loss: 1576.9795 - val_accuracy: 0.0589\n",
            "Epoch 24/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2696.7173 - accuracy: 0.0608 - val_loss: 1178.6022 - val_accuracy: 0.0663\n",
            "Epoch 25/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2043.7416 - accuracy: 0.0621 - val_loss: 771.5961 - val_accuracy: 0.0729\n",
            "Epoch 26/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 1423.1809 - accuracy: 0.0608 - val_loss: 503.5109 - val_accuracy: 0.0450\n",
            "Epoch 27/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 932.7167 - accuracy: 0.0602 - val_loss: 310.4246 - val_accuracy: 0.0537\n",
            "Epoch 28/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 503.9274 - accuracy: 0.0692 - val_loss: 165.6019 - val_accuracy: 0.0463\n",
            "Epoch 29/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 307.2808 - accuracy: 0.0759 - val_loss: 119.7460 - val_accuracy: 0.0541\n",
            "Epoch 30/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 235.7131 - accuracy: 0.0769 - val_loss: 107.9180 - val_accuracy: 0.0598\n",
            "Epoch 31/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 178.7957 - accuracy: 0.0805 - val_loss: 95.0831 - val_accuracy: 0.0541\n",
            "Epoch 32/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 143.6593 - accuracy: 0.0803 - val_loss: 87.8578 - val_accuracy: 0.0602\n",
            "Epoch 33/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 136.7856 - accuracy: 0.0826 - val_loss: 86.8231 - val_accuracy: 0.0458\n",
            "Epoch 34/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 130.4959 - accuracy: 0.0836 - val_loss: 78.2657 - val_accuracy: 0.0716\n",
            "Epoch 35/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 124.3354 - accuracy: 0.0852 - val_loss: 75.0870 - val_accuracy: 0.0738\n",
            "Epoch 36/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 118.3895 - accuracy: 0.0868 - val_loss: 73.2420 - val_accuracy: 0.0777\n",
            "Epoch 37/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 112.5128 - accuracy: 0.0873 - val_loss: 70.0958 - val_accuracy: 0.0716\n",
            "Epoch 38/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 106.7835 - accuracy: 0.0904 - val_loss: 63.1216 - val_accuracy: 0.0794\n",
            "Epoch 39/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 101.3818 - accuracy: 0.0912 - val_loss: 63.2823 - val_accuracy: 0.0685\n",
            "Epoch 40/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 96.2299 - accuracy: 0.0937 - val_loss: 59.4409 - val_accuracy: 0.0803\n",
            "Epoch 41/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 91.1476 - accuracy: 0.0964 - val_loss: 57.1462 - val_accuracy: 0.0751\n",
            "Epoch 42/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 86.4314 - accuracy: 0.0990 - val_loss: 53.7609 - val_accuracy: 0.0877\n",
            "Epoch 43/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 81.9849 - accuracy: 0.1017 - val_loss: 51.0388 - val_accuracy: 0.1026\n",
            "Epoch 44/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 77.7889 - accuracy: 0.1046 - val_loss: 49.4808 - val_accuracy: 0.0965\n",
            "Epoch 45/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 73.7870 - accuracy: 0.1090 - val_loss: 47.0791 - val_accuracy: 0.1096\n",
            "Epoch 46/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 70.0866 - accuracy: 0.1122 - val_loss: 45.1962 - val_accuracy: 0.1100\n",
            "Epoch 47/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 66.6150 - accuracy: 0.1145 - val_loss: 43.0616 - val_accuracy: 0.1091\n",
            "Epoch 48/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 63.4125 - accuracy: 0.1178 - val_loss: 40.6506 - val_accuracy: 0.1152\n",
            "Epoch 49/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 60.3524 - accuracy: 0.1220 - val_loss: 39.9254 - val_accuracy: 0.1091\n",
            "Epoch 50/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 57.4849 - accuracy: 0.1246 - val_loss: 37.3344 - val_accuracy: 0.1104\n",
            "Epoch 51/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 54.8294 - accuracy: 0.1295 - val_loss: 36.6200 - val_accuracy: 0.1174\n",
            "Epoch 52/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 52.4090 - accuracy: 0.1304 - val_loss: 35.7528 - val_accuracy: 0.1157\n",
            "Epoch 53/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 50.1260 - accuracy: 0.1342 - val_loss: 33.2474 - val_accuracy: 0.1323\n",
            "Epoch 54/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 47.8979 - accuracy: 0.1352 - val_loss: 32.2541 - val_accuracy: 0.1113\n",
            "Epoch 55/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 45.8676 - accuracy: 0.1377 - val_loss: 31.7558 - val_accuracy: 0.1213\n",
            "Epoch 56/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 43.8969 - accuracy: 0.1422 - val_loss: 29.4701 - val_accuracy: 0.1318\n",
            "Epoch 57/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 42.0442 - accuracy: 0.1445 - val_loss: 28.9616 - val_accuracy: 0.1170\n",
            "Epoch 58/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 40.2093 - accuracy: 0.1465 - val_loss: 27.0961 - val_accuracy: 0.1292\n",
            "Epoch 59/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 38.4717 - accuracy: 0.1501 - val_loss: 26.8500 - val_accuracy: 0.1187\n",
            "Epoch 60/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 36.7840 - accuracy: 0.1491 - val_loss: 26.0935 - val_accuracy: 0.1305\n",
            "Epoch 61/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 35.1594 - accuracy: 0.1521 - val_loss: 24.7647 - val_accuracy: 0.1292\n",
            "Epoch 62/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 33.4876 - accuracy: 0.1544 - val_loss: 24.1058 - val_accuracy: 0.1240\n",
            "Epoch 63/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 31.9627 - accuracy: 0.1537 - val_loss: 22.6272 - val_accuracy: 0.1349\n",
            "Epoch 64/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 30.4337 - accuracy: 0.1591 - val_loss: 21.2963 - val_accuracy: 0.1301\n",
            "Epoch 65/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 28.9329 - accuracy: 0.1609 - val_loss: 20.3378 - val_accuracy: 0.1419\n",
            "Epoch 66/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 27.5574 - accuracy: 0.1605 - val_loss: 19.3832 - val_accuracy: 0.1419\n",
            "Epoch 67/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 26.1433 - accuracy: 0.1608 - val_loss: 18.3905 - val_accuracy: 0.1375\n",
            "Epoch 68/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 24.7691 - accuracy: 0.1637 - val_loss: 17.5640 - val_accuracy: 0.1532\n",
            "Epoch 69/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 23.4692 - accuracy: 0.1656 - val_loss: 17.3794 - val_accuracy: 0.1432\n",
            "Epoch 70/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 22.2843 - accuracy: 0.1656 - val_loss: 16.1867 - val_accuracy: 0.1414\n",
            "Epoch 71/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 21.0176 - accuracy: 0.1650 - val_loss: 15.4907 - val_accuracy: 0.1589\n",
            "Epoch 72/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 19.8779 - accuracy: 0.1693 - val_loss: 14.9487 - val_accuracy: 0.1440\n",
            "Epoch 73/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 18.8134 - accuracy: 0.1695 - val_loss: 15.1656 - val_accuracy: 0.1375\n",
            "Epoch 74/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 17.6901 - accuracy: 0.1730 - val_loss: 13.7445 - val_accuracy: 0.1571\n",
            "Epoch 75/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 16.7052 - accuracy: 0.1721 - val_loss: 12.7594 - val_accuracy: 0.1550\n",
            "Epoch 76/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 15.7546 - accuracy: 0.1734 - val_loss: 12.5775 - val_accuracy: 0.1414\n",
            "Epoch 77/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 14.7960 - accuracy: 0.1741 - val_loss: 12.2969 - val_accuracy: 0.1423\n",
            "Epoch 78/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 14.0239 - accuracy: 0.1732 - val_loss: 12.0724 - val_accuracy: 0.1510\n",
            "Epoch 79/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 13.2898 - accuracy: 0.1756 - val_loss: 11.1534 - val_accuracy: 0.1515\n",
            "Epoch 80/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 12.5904 - accuracy: 0.1738 - val_loss: 10.7065 - val_accuracy: 0.1558\n",
            "Epoch 81/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 11.9927 - accuracy: 0.1753 - val_loss: 10.3786 - val_accuracy: 0.1663\n",
            "Epoch 82/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 11.4770 - accuracy: 0.1746 - val_loss: 10.6103 - val_accuracy: 0.1384\n",
            "Epoch 83/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 11.0836 - accuracy: 0.1716 - val_loss: 9.9355 - val_accuracy: 0.1663\n",
            "Epoch 84/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 10.6672 - accuracy: 0.1746 - val_loss: 9.1688 - val_accuracy: 0.1567\n",
            "Epoch 85/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 10.3359 - accuracy: 0.1743 - val_loss: 8.7709 - val_accuracy: 0.1628\n",
            "Epoch 86/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 10.0725 - accuracy: 0.1765 - val_loss: 9.4331 - val_accuracy: 0.1432\n",
            "Epoch 87/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 9.8047 - accuracy: 0.1758 - val_loss: 8.6378 - val_accuracy: 0.1619\n",
            "Epoch 88/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 9.5949 - accuracy: 0.1775 - val_loss: 8.3075 - val_accuracy: 0.1654\n",
            "Epoch 89/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 9.3668 - accuracy: 0.1760 - val_loss: 8.8543 - val_accuracy: 0.1484\n",
            "Epoch 90/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 9.1641 - accuracy: 0.1798 - val_loss: 8.1986 - val_accuracy: 0.1707\n",
            "Epoch 91/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 9.0039 - accuracy: 0.1789 - val_loss: 8.1665 - val_accuracy: 0.1427\n",
            "Epoch 92/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 8.7790 - accuracy: 0.1806 - val_loss: 8.2977 - val_accuracy: 0.1353\n",
            "Epoch 93/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 8.6263 - accuracy: 0.1852 - val_loss: 7.5075 - val_accuracy: 0.1689\n",
            "Epoch 94/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 8.4647 - accuracy: 0.1836 - val_loss: 7.8079 - val_accuracy: 0.1851\n",
            "Epoch 95/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 8.2967 - accuracy: 0.1860 - val_loss: 7.5122 - val_accuracy: 0.1646\n",
            "Epoch 96/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 8.1496 - accuracy: 0.1851 - val_loss: 7.2804 - val_accuracy: 0.1488\n",
            "Epoch 97/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 7.9668 - accuracy: 0.1880 - val_loss: 7.9740 - val_accuracy: 0.1580\n",
            "Epoch 98/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 7.8718 - accuracy: 0.1850 - val_loss: 7.1764 - val_accuracy: 0.1619\n",
            "Epoch 99/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 7.7286 - accuracy: 0.1889 - val_loss: 7.3555 - val_accuracy: 0.1729\n",
            "Epoch 100/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 7.5769 - accuracy: 0.1939 - val_loss: 6.9936 - val_accuracy: 0.1650\n",
            "Epoch 101/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 7.4539 - accuracy: 0.1935 - val_loss: 6.8513 - val_accuracy: 0.1790\n",
            "Epoch 102/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 7.3279 - accuracy: 0.1939 - val_loss: 6.6151 - val_accuracy: 0.1859\n",
            "Epoch 103/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 7.2164 - accuracy: 0.1950 - val_loss: 6.8708 - val_accuracy: 0.1746\n",
            "Epoch 104/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 7.1223 - accuracy: 0.1953 - val_loss: 7.1254 - val_accuracy: 0.1571\n",
            "Epoch 105/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 7.0244 - accuracy: 0.1960 - val_loss: 6.3908 - val_accuracy: 0.1632\n",
            "Epoch 106/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 6.8846 - accuracy: 0.1945 - val_loss: 7.1710 - val_accuracy: 0.1554\n",
            "Epoch 107/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 6.7579 - accuracy: 0.1994 - val_loss: 6.2685 - val_accuracy: 0.1829\n",
            "Epoch 108/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 6.6850 - accuracy: 0.2003 - val_loss: 6.3631 - val_accuracy: 0.1790\n",
            "Epoch 109/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 6.5859 - accuracy: 0.2020 - val_loss: 6.1048 - val_accuracy: 0.1532\n",
            "Epoch 110/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 6.5008 - accuracy: 0.2008 - val_loss: 6.0855 - val_accuracy: 0.1619\n",
            "Epoch 111/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 6.4188 - accuracy: 0.2028 - val_loss: 5.9916 - val_accuracy: 0.2003\n",
            "Epoch 112/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 6.3439 - accuracy: 0.2022 - val_loss: 6.7323 - val_accuracy: 0.1475\n",
            "Epoch 113/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 6.2598 - accuracy: 0.2038 - val_loss: 7.0287 - val_accuracy: 0.1371\n",
            "Epoch 114/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 6.1921 - accuracy: 0.2020 - val_loss: 6.3308 - val_accuracy: 0.1785\n",
            "Epoch 115/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 6.1211 - accuracy: 0.2048 - val_loss: 6.5141 - val_accuracy: 0.1523\n",
            "Epoch 116/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 6.0620 - accuracy: 0.2038 - val_loss: 5.7392 - val_accuracy: 0.1807\n",
            "Epoch 117/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 5.9657 - accuracy: 0.2084 - val_loss: 5.5321 - val_accuracy: 0.1825\n",
            "Epoch 118/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 5.9055 - accuracy: 0.2092 - val_loss: 6.0712 - val_accuracy: 0.1598\n",
            "Epoch 119/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 5.8546 - accuracy: 0.2103 - val_loss: 5.3750 - val_accuracy: 0.1855\n",
            "Epoch 120/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 5.7952 - accuracy: 0.2067 - val_loss: 6.1058 - val_accuracy: 0.1532\n",
            "Epoch 121/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 5.7464 - accuracy: 0.2108 - val_loss: 5.5739 - val_accuracy: 0.1999\n",
            "Epoch 122/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 5.7180 - accuracy: 0.2117 - val_loss: 5.3541 - val_accuracy: 0.2038\n",
            "Epoch 123/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 5.6307 - accuracy: 0.2109 - val_loss: 5.4802 - val_accuracy: 0.1750\n",
            "Epoch 124/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 5.5703 - accuracy: 0.2100 - val_loss: 5.6944 - val_accuracy: 0.1532\n",
            "Epoch 125/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 5.5517 - accuracy: 0.2124 - val_loss: 5.5452 - val_accuracy: 0.2021\n",
            "Epoch 126/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 5.4834 - accuracy: 0.2141 - val_loss: 5.4221 - val_accuracy: 0.1886\n",
            "Epoch 127/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 5.4614 - accuracy: 0.2133 - val_loss: 5.8501 - val_accuracy: 0.1602\n",
            "Epoch 128/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 5.4046 - accuracy: 0.2123 - val_loss: 5.0904 - val_accuracy: 0.1977\n",
            "Epoch 129/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 5.3385 - accuracy: 0.2145 - val_loss: 5.1811 - val_accuracy: 0.1724\n",
            "Epoch 130/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 5.3073 - accuracy: 0.2164 - val_loss: 5.1469 - val_accuracy: 0.1820\n",
            "Epoch 131/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 5.2535 - accuracy: 0.2138 - val_loss: 4.8260 - val_accuracy: 0.1811\n",
            "Epoch 132/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 5.2025 - accuracy: 0.2174 - val_loss: 4.9007 - val_accuracy: 0.1916\n",
            "Epoch 133/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 5.2030 - accuracy: 0.2150 - val_loss: 5.3895 - val_accuracy: 0.1707\n",
            "Epoch 134/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 5.1224 - accuracy: 0.2167 - val_loss: 4.8947 - val_accuracy: 0.2025\n",
            "Epoch 135/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 5.0727 - accuracy: 0.2204 - val_loss: 5.2554 - val_accuracy: 0.1903\n",
            "Epoch 136/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 5.0560 - accuracy: 0.2185 - val_loss: 4.7538 - val_accuracy: 0.2143\n",
            "Epoch 137/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 5.0022 - accuracy: 0.2202 - val_loss: 5.0348 - val_accuracy: 0.2161\n",
            "Epoch 138/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.9665 - accuracy: 0.2193 - val_loss: 5.0429 - val_accuracy: 0.1694\n",
            "Epoch 139/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.9489 - accuracy: 0.2198 - val_loss: 5.1865 - val_accuracy: 0.1947\n",
            "Epoch 140/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.9039 - accuracy: 0.2234 - val_loss: 5.0053 - val_accuracy: 0.1545\n",
            "Epoch 141/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.8672 - accuracy: 0.2202 - val_loss: 5.0207 - val_accuracy: 0.1890\n",
            "Epoch 142/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.8236 - accuracy: 0.2248 - val_loss: 4.5559 - val_accuracy: 0.2152\n",
            "Epoch 143/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.7801 - accuracy: 0.2241 - val_loss: 4.9999 - val_accuracy: 0.1724\n",
            "Epoch 144/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.7613 - accuracy: 0.2231 - val_loss: 4.3881 - val_accuracy: 0.1969\n",
            "Epoch 145/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.7265 - accuracy: 0.2264 - val_loss: 4.5730 - val_accuracy: 0.2100\n",
            "Epoch 146/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.6597 - accuracy: 0.2263 - val_loss: 4.8987 - val_accuracy: 0.1916\n",
            "Epoch 147/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.6500 - accuracy: 0.2269 - val_loss: 4.5207 - val_accuracy: 0.1886\n",
            "Epoch 148/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.6214 - accuracy: 0.2244 - val_loss: 4.4673 - val_accuracy: 0.2235\n",
            "Epoch 149/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.5891 - accuracy: 0.2287 - val_loss: 4.4007 - val_accuracy: 0.2182\n",
            "Epoch 150/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.5545 - accuracy: 0.2281 - val_loss: 4.5504 - val_accuracy: 0.1785\n",
            "Epoch 151/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.5326 - accuracy: 0.2257 - val_loss: 4.7373 - val_accuracy: 0.2003\n",
            "Epoch 152/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 4.4958 - accuracy: 0.2299 - val_loss: 4.9689 - val_accuracy: 0.1986\n",
            "Epoch 153/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 4.4607 - accuracy: 0.2312 - val_loss: 5.1659 - val_accuracy: 0.1619\n",
            "Epoch 154/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.4395 - accuracy: 0.2283 - val_loss: 4.3963 - val_accuracy: 0.2104\n",
            "Epoch 155/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.4108 - accuracy: 0.2286 - val_loss: 4.1990 - val_accuracy: 0.2230\n",
            "Epoch 156/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.3981 - accuracy: 0.2313 - val_loss: 5.0234 - val_accuracy: 0.1711\n",
            "Epoch 157/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 4.3490 - accuracy: 0.2284 - val_loss: 4.3433 - val_accuracy: 0.2117\n",
            "Epoch 158/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.3293 - accuracy: 0.2322 - val_loss: 4.3656 - val_accuracy: 0.2078\n",
            "Epoch 159/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.3013 - accuracy: 0.2331 - val_loss: 4.5242 - val_accuracy: 0.2318\n",
            "Epoch 160/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 4.2628 - accuracy: 0.2336 - val_loss: 4.4744 - val_accuracy: 0.1659\n",
            "Epoch 161/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 4.2651 - accuracy: 0.2315 - val_loss: 4.5813 - val_accuracy: 0.1742\n",
            "Epoch 162/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.2339 - accuracy: 0.2339 - val_loss: 4.0602 - val_accuracy: 0.2161\n",
            "Epoch 163/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 4.1847 - accuracy: 0.2362 - val_loss: 4.4064 - val_accuracy: 0.1794\n",
            "Epoch 164/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.1835 - accuracy: 0.2326 - val_loss: 4.2660 - val_accuracy: 0.2527\n",
            "Epoch 165/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.1642 - accuracy: 0.2333 - val_loss: 4.1519 - val_accuracy: 0.2139\n",
            "Epoch 166/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.1194 - accuracy: 0.2342 - val_loss: 4.2707 - val_accuracy: 0.2165\n",
            "Epoch 167/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.1214 - accuracy: 0.2370 - val_loss: 4.5514 - val_accuracy: 0.1829\n",
            "Epoch 168/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.0886 - accuracy: 0.2373 - val_loss: 3.9026 - val_accuracy: 0.2405\n",
            "Epoch 169/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.0846 - accuracy: 0.2373 - val_loss: 4.2052 - val_accuracy: 0.1667\n",
            "Epoch 170/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 4.0349 - accuracy: 0.2354 - val_loss: 3.8028 - val_accuracy: 0.2156\n",
            "Epoch 171/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.0269 - accuracy: 0.2403 - val_loss: 3.8821 - val_accuracy: 0.2318\n",
            "Epoch 172/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 4.0001 - accuracy: 0.2351 - val_loss: 3.8759 - val_accuracy: 0.2086\n",
            "Epoch 173/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 4.0026 - accuracy: 0.2355 - val_loss: 4.3197 - val_accuracy: 0.2174\n",
            "Epoch 174/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.9845 - accuracy: 0.2374 - val_loss: 4.5301 - val_accuracy: 0.2021\n",
            "Epoch 175/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.9628 - accuracy: 0.2376 - val_loss: 3.8412 - val_accuracy: 0.2278\n",
            "Epoch 176/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.9154 - accuracy: 0.2379 - val_loss: 4.2368 - val_accuracy: 0.1990\n",
            "Epoch 177/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.9095 - accuracy: 0.2391 - val_loss: 3.9480 - val_accuracy: 0.1916\n",
            "Epoch 178/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.8947 - accuracy: 0.2423 - val_loss: 4.3560 - val_accuracy: 0.1550\n",
            "Epoch 179/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.8902 - accuracy: 0.2403 - val_loss: 3.8745 - val_accuracy: 0.2296\n",
            "Epoch 180/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 3.8463 - accuracy: 0.2419 - val_loss: 4.2310 - val_accuracy: 0.1628\n",
            "Epoch 181/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.8316 - accuracy: 0.2407 - val_loss: 4.0900 - val_accuracy: 0.1934\n",
            "Epoch 182/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.8261 - accuracy: 0.2448 - val_loss: 4.0380 - val_accuracy: 0.2021\n",
            "Epoch 183/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.8066 - accuracy: 0.2391 - val_loss: 3.7858 - val_accuracy: 0.2178\n",
            "Epoch 184/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 3.7869 - accuracy: 0.2440 - val_loss: 4.0395 - val_accuracy: 0.1781\n",
            "Epoch 185/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 3.7605 - accuracy: 0.2456 - val_loss: 3.6198 - val_accuracy: 0.2527\n",
            "Epoch 186/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 3.7511 - accuracy: 0.2470 - val_loss: 3.4356 - val_accuracy: 0.2466\n",
            "Epoch 187/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 3.7401 - accuracy: 0.2441 - val_loss: 4.0391 - val_accuracy: 0.1890\n",
            "Epoch 188/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 3.7070 - accuracy: 0.2447 - val_loss: 4.0099 - val_accuracy: 0.2104\n",
            "Epoch 189/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.7130 - accuracy: 0.2451 - val_loss: 3.8382 - val_accuracy: 0.2065\n",
            "Epoch 190/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.6833 - accuracy: 0.2472 - val_loss: 3.4699 - val_accuracy: 0.2593\n",
            "Epoch 191/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.6641 - accuracy: 0.2462 - val_loss: 3.4845 - val_accuracy: 0.2562\n",
            "Epoch 192/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.6646 - accuracy: 0.2474 - val_loss: 3.6176 - val_accuracy: 0.2139\n",
            "Epoch 193/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 3.6179 - accuracy: 0.2462 - val_loss: 3.4236 - val_accuracy: 0.2571\n",
            "Epoch 194/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 3.6267 - accuracy: 0.2464 - val_loss: 4.1640 - val_accuracy: 0.1576\n",
            "Epoch 195/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.6075 - accuracy: 0.2460 - val_loss: 4.4333 - val_accuracy: 0.1550\n",
            "Epoch 196/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.5898 - accuracy: 0.2477 - val_loss: 3.5244 - val_accuracy: 0.2204\n",
            "Epoch 197/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.5850 - accuracy: 0.2457 - val_loss: 3.4852 - val_accuracy: 0.2252\n",
            "Epoch 198/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.5672 - accuracy: 0.2491 - val_loss: 3.7633 - val_accuracy: 0.1999\n",
            "Epoch 199/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.5531 - accuracy: 0.2483 - val_loss: 3.5698 - val_accuracy: 0.2152\n",
            "Epoch 200/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.5407 - accuracy: 0.2459 - val_loss: 3.3902 - val_accuracy: 0.2405\n",
            "Epoch 201/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.5278 - accuracy: 0.2487 - val_loss: 3.5636 - val_accuracy: 0.2169\n",
            "Epoch 202/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.4970 - accuracy: 0.2470 - val_loss: 3.5176 - val_accuracy: 0.2366\n",
            "Epoch 203/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.4776 - accuracy: 0.2504 - val_loss: 3.4205 - val_accuracy: 0.2379\n",
            "Epoch 204/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 3.4968 - accuracy: 0.2483 - val_loss: 3.7859 - val_accuracy: 0.1886\n",
            "Epoch 205/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.4691 - accuracy: 0.2483 - val_loss: 4.0139 - val_accuracy: 0.1855\n",
            "Epoch 206/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 3.4520 - accuracy: 0.2513 - val_loss: 3.6220 - val_accuracy: 0.2209\n",
            "Epoch 207/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.4485 - accuracy: 0.2489 - val_loss: 4.1825 - val_accuracy: 0.1632\n",
            "Epoch 208/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.4481 - accuracy: 0.2506 - val_loss: 3.5467 - val_accuracy: 0.2065\n",
            "Epoch 209/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.4187 - accuracy: 0.2508 - val_loss: 3.2957 - val_accuracy: 0.2405\n",
            "Epoch 210/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.4116 - accuracy: 0.2513 - val_loss: 3.7532 - val_accuracy: 0.1912\n",
            "Epoch 211/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.3964 - accuracy: 0.2514 - val_loss: 3.2849 - val_accuracy: 0.2309\n",
            "Epoch 212/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.3926 - accuracy: 0.2533 - val_loss: 3.3908 - val_accuracy: 0.2148\n",
            "Epoch 213/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.3612 - accuracy: 0.2525 - val_loss: 3.4978 - val_accuracy: 0.2113\n",
            "Epoch 214/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 3.3625 - accuracy: 0.2519 - val_loss: 3.6370 - val_accuracy: 0.1894\n",
            "Epoch 215/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.3511 - accuracy: 0.2527 - val_loss: 3.4120 - val_accuracy: 0.2117\n",
            "Epoch 216/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 3.3412 - accuracy: 0.2523 - val_loss: 3.5645 - val_accuracy: 0.2235\n",
            "Epoch 217/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.3325 - accuracy: 0.2553 - val_loss: 3.7544 - val_accuracy: 0.2056\n",
            "Epoch 218/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 3.3005 - accuracy: 0.2572 - val_loss: 3.4844 - val_accuracy: 0.2615\n",
            "Epoch 219/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.2967 - accuracy: 0.2553 - val_loss: 3.1803 - val_accuracy: 0.2466\n",
            "Epoch 220/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.2920 - accuracy: 0.2533 - val_loss: 3.7455 - val_accuracy: 0.2156\n",
            "Epoch 221/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.2766 - accuracy: 0.2564 - val_loss: 4.1784 - val_accuracy: 0.1606\n",
            "Epoch 222/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.2795 - accuracy: 0.2548 - val_loss: 3.0983 - val_accuracy: 0.2562\n",
            "Epoch 223/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.2600 - accuracy: 0.2566 - val_loss: 3.5081 - val_accuracy: 0.2305\n",
            "Epoch 224/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.2588 - accuracy: 0.2563 - val_loss: 3.2594 - val_accuracy: 0.2021\n",
            "Epoch 225/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 3.2317 - accuracy: 0.2559 - val_loss: 3.5750 - val_accuracy: 0.2423\n",
            "Epoch 226/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.2304 - accuracy: 0.2552 - val_loss: 4.6176 - val_accuracy: 0.1541\n",
            "Epoch 227/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.2066 - accuracy: 0.2520 - val_loss: 3.1537 - val_accuracy: 0.2457\n",
            "Epoch 228/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.1992 - accuracy: 0.2547 - val_loss: 3.5234 - val_accuracy: 0.1894\n",
            "Epoch 229/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.1987 - accuracy: 0.2546 - val_loss: 3.6092 - val_accuracy: 0.1750\n",
            "Epoch 230/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.1636 - accuracy: 0.2592 - val_loss: 3.5034 - val_accuracy: 0.2003\n",
            "Epoch 231/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 3.1706 - accuracy: 0.2589 - val_loss: 3.6459 - val_accuracy: 0.1689\n",
            "Epoch 232/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.1663 - accuracy: 0.2577 - val_loss: 3.2068 - val_accuracy: 0.2003\n",
            "Epoch 233/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.1466 - accuracy: 0.2585 - val_loss: 3.7239 - val_accuracy: 0.1951\n",
            "Epoch 234/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.1500 - accuracy: 0.2581 - val_loss: 3.5730 - val_accuracy: 0.2100\n",
            "Epoch 235/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.1290 - accuracy: 0.2578 - val_loss: 3.2295 - val_accuracy: 0.2143\n",
            "Epoch 236/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 3.1452 - accuracy: 0.2580 - val_loss: 3.3704 - val_accuracy: 0.2078\n",
            "Epoch 237/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 3.1277 - accuracy: 0.2603 - val_loss: 3.1159 - val_accuracy: 0.2510\n",
            "Epoch 238/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.1196 - accuracy: 0.2597 - val_loss: 3.3985 - val_accuracy: 0.2475\n",
            "Epoch 239/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.1004 - accuracy: 0.2595 - val_loss: 3.3158 - val_accuracy: 0.2152\n",
            "Epoch 240/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.1301 - accuracy: 0.2562 - val_loss: 3.1071 - val_accuracy: 0.2292\n",
            "Epoch 241/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.0671 - accuracy: 0.2612 - val_loss: 4.6346 - val_accuracy: 0.1388\n",
            "Epoch 242/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.0746 - accuracy: 0.2600 - val_loss: 4.1278 - val_accuracy: 0.1200\n",
            "Epoch 243/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 3.0697 - accuracy: 0.2584 - val_loss: 3.2759 - val_accuracy: 0.2239\n",
            "Epoch 244/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.0803 - accuracy: 0.2607 - val_loss: 3.2766 - val_accuracy: 0.2296\n",
            "Epoch 245/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.0714 - accuracy: 0.2599 - val_loss: 3.1597 - val_accuracy: 0.2405\n",
            "Epoch 246/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.0284 - accuracy: 0.2623 - val_loss: 2.9928 - val_accuracy: 0.2174\n",
            "Epoch 247/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 3.0244 - accuracy: 0.2616 - val_loss: 2.8965 - val_accuracy: 0.2663\n",
            "Epoch 248/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 3.0141 - accuracy: 0.2601 - val_loss: 3.0345 - val_accuracy: 0.2728\n",
            "Epoch 249/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.0313 - accuracy: 0.2613 - val_loss: 3.9394 - val_accuracy: 0.1611\n",
            "Epoch 250/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.0259 - accuracy: 0.2634 - val_loss: 3.6089 - val_accuracy: 0.1737\n",
            "Epoch 251/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.9916 - accuracy: 0.2615 - val_loss: 3.2926 - val_accuracy: 0.1929\n",
            "Epoch 252/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.9954 - accuracy: 0.2590 - val_loss: 3.1299 - val_accuracy: 0.2562\n",
            "Epoch 253/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 3.0018 - accuracy: 0.2604 - val_loss: 3.5801 - val_accuracy: 0.1803\n",
            "Epoch 254/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.9795 - accuracy: 0.2660 - val_loss: 3.0034 - val_accuracy: 0.2392\n",
            "Epoch 255/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.9779 - accuracy: 0.2642 - val_loss: 2.9720 - val_accuracy: 0.2353\n",
            "Epoch 256/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.9640 - accuracy: 0.2635 - val_loss: 3.1915 - val_accuracy: 0.2148\n",
            "Epoch 257/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.9694 - accuracy: 0.2635 - val_loss: 4.3016 - val_accuracy: 0.1375\n",
            "Epoch 258/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.9770 - accuracy: 0.2626 - val_loss: 3.1254 - val_accuracy: 0.2169\n",
            "Epoch 259/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.9468 - accuracy: 0.2638 - val_loss: 3.3052 - val_accuracy: 0.2440\n",
            "Epoch 260/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.9347 - accuracy: 0.2615 - val_loss: 2.8245 - val_accuracy: 0.2601\n",
            "Epoch 261/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.9118 - accuracy: 0.2647 - val_loss: 3.2332 - val_accuracy: 0.2117\n",
            "Epoch 262/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.9291 - accuracy: 0.2655 - val_loss: 3.0643 - val_accuracy: 0.2488\n",
            "Epoch 263/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.9040 - accuracy: 0.2647 - val_loss: 3.0941 - val_accuracy: 0.2471\n",
            "Epoch 264/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.8892 - accuracy: 0.2688 - val_loss: 3.1676 - val_accuracy: 0.2423\n",
            "Epoch 265/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.9207 - accuracy: 0.2663 - val_loss: 3.2165 - val_accuracy: 0.1982\n",
            "Epoch 266/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.8874 - accuracy: 0.2649 - val_loss: 3.3136 - val_accuracy: 0.2344\n",
            "Epoch 267/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.8804 - accuracy: 0.2677 - val_loss: 2.9543 - val_accuracy: 0.2597\n",
            "Epoch 268/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.8715 - accuracy: 0.2674 - val_loss: 3.0662 - val_accuracy: 0.2148\n",
            "Epoch 269/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.9067 - accuracy: 0.2660 - val_loss: 3.7951 - val_accuracy: 0.2305\n",
            "Epoch 270/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.8751 - accuracy: 0.2675 - val_loss: 3.6332 - val_accuracy: 0.1720\n",
            "Epoch 271/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.9063 - accuracy: 0.2664 - val_loss: 3.2014 - val_accuracy: 0.2261\n",
            "Epoch 272/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.8668 - accuracy: 0.2654 - val_loss: 3.0852 - val_accuracy: 0.2366\n",
            "Epoch 273/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.8664 - accuracy: 0.2664 - val_loss: 2.7879 - val_accuracy: 0.2540\n",
            "Epoch 274/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.8455 - accuracy: 0.2682 - val_loss: 2.9976 - val_accuracy: 0.2414\n",
            "Epoch 275/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.8257 - accuracy: 0.2724 - val_loss: 3.3176 - val_accuracy: 0.1785\n",
            "Epoch 276/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.8244 - accuracy: 0.2671 - val_loss: 3.2049 - val_accuracy: 0.2226\n",
            "Epoch 277/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.8487 - accuracy: 0.2616 - val_loss: 3.5516 - val_accuracy: 0.2069\n",
            "Epoch 278/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.8199 - accuracy: 0.2694 - val_loss: 2.8947 - val_accuracy: 0.2772\n",
            "Epoch 279/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.8338 - accuracy: 0.2675 - val_loss: 3.5839 - val_accuracy: 0.2104\n",
            "Epoch 280/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.8176 - accuracy: 0.2664 - val_loss: 2.9528 - val_accuracy: 0.2309\n",
            "Epoch 281/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.8194 - accuracy: 0.2701 - val_loss: 3.4416 - val_accuracy: 0.2239\n",
            "Epoch 282/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.7887 - accuracy: 0.2699 - val_loss: 4.0479 - val_accuracy: 0.1148\n",
            "Epoch 283/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.8135 - accuracy: 0.2681 - val_loss: 3.3823 - val_accuracy: 0.2270\n",
            "Epoch 284/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.7935 - accuracy: 0.2674 - val_loss: 3.0571 - val_accuracy: 0.2257\n",
            "Epoch 285/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.7863 - accuracy: 0.2706 - val_loss: 3.1385 - val_accuracy: 0.2667\n",
            "Epoch 286/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.7803 - accuracy: 0.2715 - val_loss: 2.9102 - val_accuracy: 0.2523\n",
            "Epoch 287/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.7671 - accuracy: 0.2762 - val_loss: 2.9879 - val_accuracy: 0.2562\n",
            "Epoch 288/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.7722 - accuracy: 0.2707 - val_loss: 3.3397 - val_accuracy: 0.2003\n",
            "Epoch 289/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.7590 - accuracy: 0.2709 - val_loss: 3.3698 - val_accuracy: 0.2152\n",
            "Epoch 290/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.7544 - accuracy: 0.2693 - val_loss: 3.4233 - val_accuracy: 0.1576\n",
            "Epoch 291/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.7570 - accuracy: 0.2692 - val_loss: 3.2139 - val_accuracy: 0.2366\n",
            "Epoch 292/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.7524 - accuracy: 0.2710 - val_loss: 3.6613 - val_accuracy: 0.1781\n",
            "Epoch 293/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.7654 - accuracy: 0.2722 - val_loss: 2.8485 - val_accuracy: 0.2414\n",
            "Epoch 294/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.7402 - accuracy: 0.2726 - val_loss: 3.1125 - val_accuracy: 0.2383\n",
            "Epoch 295/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.7380 - accuracy: 0.2722 - val_loss: 3.1752 - val_accuracy: 0.2820\n",
            "Epoch 296/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.7238 - accuracy: 0.2736 - val_loss: 3.3407 - val_accuracy: 0.2226\n",
            "Epoch 297/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.7303 - accuracy: 0.2726 - val_loss: 3.3997 - val_accuracy: 0.2091\n",
            "Epoch 298/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.7489 - accuracy: 0.2752 - val_loss: 3.0501 - val_accuracy: 0.2326\n",
            "Epoch 299/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.7210 - accuracy: 0.2774 - val_loss: 3.1187 - val_accuracy: 0.1859\n",
            "Epoch 300/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.6966 - accuracy: 0.2714 - val_loss: 3.0194 - val_accuracy: 0.2689\n",
            "Epoch 301/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.7370 - accuracy: 0.2715 - val_loss: 3.1539 - val_accuracy: 0.2505\n",
            "Epoch 302/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.7027 - accuracy: 0.2744 - val_loss: 3.0032 - val_accuracy: 0.2235\n",
            "Epoch 303/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.6823 - accuracy: 0.2766 - val_loss: 2.8992 - val_accuracy: 0.2706\n",
            "Epoch 304/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.6983 - accuracy: 0.2760 - val_loss: 2.9538 - val_accuracy: 0.2191\n",
            "Epoch 305/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.6998 - accuracy: 0.2767 - val_loss: 3.0912 - val_accuracy: 0.1903\n",
            "Epoch 306/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.6986 - accuracy: 0.2759 - val_loss: 3.1543 - val_accuracy: 0.1807\n",
            "Epoch 307/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.7021 - accuracy: 0.2773 - val_loss: 2.7673 - val_accuracy: 0.2811\n",
            "Epoch 308/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.6767 - accuracy: 0.2730 - val_loss: 3.1429 - val_accuracy: 0.2222\n",
            "Epoch 309/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.6834 - accuracy: 0.2740 - val_loss: 3.3001 - val_accuracy: 0.2156\n",
            "Epoch 310/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.6683 - accuracy: 0.2753 - val_loss: 3.0888 - val_accuracy: 0.2601\n",
            "Epoch 311/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.6655 - accuracy: 0.2740 - val_loss: 2.9582 - val_accuracy: 0.2497\n",
            "Epoch 312/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.6744 - accuracy: 0.2757 - val_loss: 3.1022 - val_accuracy: 0.2340\n",
            "Epoch 313/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.6789 - accuracy: 0.2769 - val_loss: 2.8905 - val_accuracy: 0.2780\n",
            "Epoch 314/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.6709 - accuracy: 0.2749 - val_loss: 3.0576 - val_accuracy: 0.2222\n",
            "Epoch 315/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.6637 - accuracy: 0.2766 - val_loss: 3.3373 - val_accuracy: 0.1641\n",
            "Epoch 316/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.6468 - accuracy: 0.2760 - val_loss: 2.8862 - val_accuracy: 0.2449\n",
            "Epoch 317/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.6546 - accuracy: 0.2748 - val_loss: 2.8765 - val_accuracy: 0.2916\n",
            "Epoch 318/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.6441 - accuracy: 0.2748 - val_loss: 2.8005 - val_accuracy: 0.2942\n",
            "Epoch 319/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.6517 - accuracy: 0.2745 - val_loss: 2.5683 - val_accuracy: 0.2850\n",
            "Epoch 320/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.6305 - accuracy: 0.2819 - val_loss: 2.9099 - val_accuracy: 0.2401\n",
            "Epoch 321/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.6261 - accuracy: 0.2789 - val_loss: 2.9461 - val_accuracy: 0.2510\n",
            "Epoch 322/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.6251 - accuracy: 0.2736 - val_loss: 2.9189 - val_accuracy: 0.2619\n",
            "Epoch 323/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.6359 - accuracy: 0.2766 - val_loss: 3.0671 - val_accuracy: 0.2130\n",
            "Epoch 324/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.6278 - accuracy: 0.2766 - val_loss: 3.1160 - val_accuracy: 0.2418\n",
            "Epoch 325/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.6345 - accuracy: 0.2789 - val_loss: 2.9176 - val_accuracy: 0.2567\n",
            "Epoch 326/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.6312 - accuracy: 0.2760 - val_loss: 3.0266 - val_accuracy: 0.2667\n",
            "Epoch 327/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.6105 - accuracy: 0.2812 - val_loss: 2.8274 - val_accuracy: 0.2549\n",
            "Epoch 328/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.6100 - accuracy: 0.2778 - val_loss: 3.0102 - val_accuracy: 0.2457\n",
            "Epoch 329/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.6177 - accuracy: 0.2824 - val_loss: 2.9516 - val_accuracy: 0.2418\n",
            "Epoch 330/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.6074 - accuracy: 0.2796 - val_loss: 3.2509 - val_accuracy: 0.2850\n",
            "Epoch 331/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.6246 - accuracy: 0.2795 - val_loss: 2.6577 - val_accuracy: 0.2575\n",
            "Epoch 332/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.6195 - accuracy: 0.2782 - val_loss: 3.3140 - val_accuracy: 0.1951\n",
            "Epoch 333/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.6123 - accuracy: 0.2788 - val_loss: 2.9457 - val_accuracy: 0.2388\n",
            "Epoch 334/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.5896 - accuracy: 0.2817 - val_loss: 2.8774 - val_accuracy: 0.2780\n",
            "Epoch 335/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5931 - accuracy: 0.2800 - val_loss: 3.1349 - val_accuracy: 0.2108\n",
            "Epoch 336/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5844 - accuracy: 0.2791 - val_loss: 2.7419 - val_accuracy: 0.2623\n",
            "Epoch 337/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.6043 - accuracy: 0.2771 - val_loss: 3.1875 - val_accuracy: 0.1807\n",
            "Epoch 338/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.5997 - accuracy: 0.2787 - val_loss: 3.0567 - val_accuracy: 0.2191\n",
            "Epoch 339/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5964 - accuracy: 0.2768 - val_loss: 2.6519 - val_accuracy: 0.2479\n",
            "Epoch 340/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5866 - accuracy: 0.2834 - val_loss: 3.0635 - val_accuracy: 0.2196\n",
            "Epoch 341/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5865 - accuracy: 0.2836 - val_loss: 2.7386 - val_accuracy: 0.3069\n",
            "Epoch 342/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5720 - accuracy: 0.2810 - val_loss: 2.7056 - val_accuracy: 0.2767\n",
            "Epoch 343/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.6006 - accuracy: 0.2746 - val_loss: 2.6357 - val_accuracy: 0.2558\n",
            "Epoch 344/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5635 - accuracy: 0.2806 - val_loss: 2.9672 - val_accuracy: 0.2003\n",
            "Epoch 345/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5665 - accuracy: 0.2819 - val_loss: 2.8167 - val_accuracy: 0.2331\n",
            "Epoch 346/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5701 - accuracy: 0.2787 - val_loss: 3.3053 - val_accuracy: 0.1763\n",
            "Epoch 347/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5601 - accuracy: 0.2834 - val_loss: 3.6157 - val_accuracy: 0.1894\n",
            "Epoch 348/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.5614 - accuracy: 0.2821 - val_loss: 3.0067 - val_accuracy: 0.2667\n",
            "Epoch 349/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5557 - accuracy: 0.2837 - val_loss: 2.9744 - val_accuracy: 0.2628\n",
            "Epoch 350/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5547 - accuracy: 0.2813 - val_loss: 2.8828 - val_accuracy: 0.2649\n",
            "Epoch 351/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5508 - accuracy: 0.2823 - val_loss: 3.3210 - val_accuracy: 0.2719\n",
            "Epoch 352/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5560 - accuracy: 0.2810 - val_loss: 3.6462 - val_accuracy: 0.2139\n",
            "Epoch 353/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5513 - accuracy: 0.2820 - val_loss: 2.9463 - val_accuracy: 0.2038\n",
            "Epoch 354/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5623 - accuracy: 0.2850 - val_loss: 3.5328 - val_accuracy: 0.1227\n",
            "Epoch 355/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.5512 - accuracy: 0.2821 - val_loss: 2.7240 - val_accuracy: 0.2671\n",
            "Epoch 356/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.5491 - accuracy: 0.2811 - val_loss: 3.1031 - val_accuracy: 0.1720\n",
            "Epoch 357/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.5980 - accuracy: 0.2829 - val_loss: 3.2912 - val_accuracy: 0.2252\n",
            "Epoch 358/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.5465 - accuracy: 0.2818 - val_loss: 2.5338 - val_accuracy: 0.2807\n",
            "Epoch 359/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5247 - accuracy: 0.2847 - val_loss: 3.0210 - val_accuracy: 0.2436\n",
            "Epoch 360/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5391 - accuracy: 0.2823 - val_loss: 2.9561 - val_accuracy: 0.2340\n",
            "Epoch 361/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5336 - accuracy: 0.2808 - val_loss: 3.2070 - val_accuracy: 0.1532\n",
            "Epoch 362/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5386 - accuracy: 0.2829 - val_loss: 3.0616 - val_accuracy: 0.1995\n",
            "Epoch 363/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.5054 - accuracy: 0.2832 - val_loss: 3.0709 - val_accuracy: 0.2082\n",
            "Epoch 364/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.5339 - accuracy: 0.2832 - val_loss: 2.8643 - val_accuracy: 0.2388\n",
            "Epoch 365/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5611 - accuracy: 0.2816 - val_loss: 2.7886 - val_accuracy: 0.2571\n",
            "Epoch 366/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.5166 - accuracy: 0.2821 - val_loss: 2.7757 - val_accuracy: 0.2776\n",
            "Epoch 367/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5146 - accuracy: 0.2797 - val_loss: 2.9071 - val_accuracy: 0.2484\n",
            "Epoch 368/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5092 - accuracy: 0.2830 - val_loss: 3.1262 - val_accuracy: 0.2012\n",
            "Epoch 369/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5219 - accuracy: 0.2871 - val_loss: 3.1781 - val_accuracy: 0.2117\n",
            "Epoch 370/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5243 - accuracy: 0.2862 - val_loss: 2.6749 - val_accuracy: 0.2636\n",
            "Epoch 371/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5153 - accuracy: 0.2848 - val_loss: 3.3141 - val_accuracy: 0.2545\n",
            "Epoch 372/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.5176 - accuracy: 0.2819 - val_loss: 2.8160 - val_accuracy: 0.2654\n",
            "Epoch 373/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5152 - accuracy: 0.2862 - val_loss: 3.0128 - val_accuracy: 0.2047\n",
            "Epoch 374/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5415 - accuracy: 0.2844 - val_loss: 3.1443 - val_accuracy: 0.2065\n",
            "Epoch 375/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.5199 - accuracy: 0.2826 - val_loss: 2.9435 - val_accuracy: 0.2903\n",
            "Epoch 376/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5114 - accuracy: 0.2823 - val_loss: 2.5031 - val_accuracy: 0.2898\n",
            "Epoch 377/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5035 - accuracy: 0.2864 - val_loss: 2.6869 - val_accuracy: 0.2881\n",
            "Epoch 378/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5075 - accuracy: 0.2840 - val_loss: 3.5912 - val_accuracy: 0.1689\n",
            "Epoch 379/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5282 - accuracy: 0.2827 - val_loss: 3.3399 - val_accuracy: 0.1715\n",
            "Epoch 380/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4965 - accuracy: 0.2896 - val_loss: 3.4464 - val_accuracy: 0.2060\n",
            "Epoch 381/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.4949 - accuracy: 0.2900 - val_loss: 3.3991 - val_accuracy: 0.1510\n",
            "Epoch 382/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4869 - accuracy: 0.2868 - val_loss: 3.0907 - val_accuracy: 0.2283\n",
            "Epoch 383/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4902 - accuracy: 0.2847 - val_loss: 2.9830 - val_accuracy: 0.2322\n",
            "Epoch 384/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.5055 - accuracy: 0.2810 - val_loss: 2.5170 - val_accuracy: 0.2850\n",
            "Epoch 385/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4769 - accuracy: 0.2866 - val_loss: 2.5625 - val_accuracy: 0.2698\n",
            "Epoch 386/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5094 - accuracy: 0.2844 - val_loss: 3.0868 - val_accuracy: 0.1842\n",
            "Epoch 387/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5051 - accuracy: 0.2832 - val_loss: 3.0679 - val_accuracy: 0.1964\n",
            "Epoch 388/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5123 - accuracy: 0.2856 - val_loss: 2.7278 - val_accuracy: 0.2575\n",
            "Epoch 389/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4688 - accuracy: 0.2865 - val_loss: 3.7495 - val_accuracy: 0.2130\n",
            "Epoch 390/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5154 - accuracy: 0.2809 - val_loss: 2.7641 - val_accuracy: 0.2842\n",
            "Epoch 391/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4951 - accuracy: 0.2813 - val_loss: 3.0715 - val_accuracy: 0.2305\n",
            "Epoch 392/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.5082 - accuracy: 0.2839 - val_loss: 4.1848 - val_accuracy: 0.1131\n",
            "Epoch 393/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4952 - accuracy: 0.2833 - val_loss: 3.3811 - val_accuracy: 0.2370\n",
            "Epoch 394/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4793 - accuracy: 0.2849 - val_loss: 2.7146 - val_accuracy: 0.2741\n",
            "Epoch 395/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.5047 - accuracy: 0.2868 - val_loss: 2.8903 - val_accuracy: 0.2553\n",
            "Epoch 396/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4961 - accuracy: 0.2853 - val_loss: 2.7164 - val_accuracy: 0.2239\n",
            "Epoch 397/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.4664 - accuracy: 0.2857 - val_loss: 3.0644 - val_accuracy: 0.2235\n",
            "Epoch 398/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4748 - accuracy: 0.2875 - val_loss: 3.4522 - val_accuracy: 0.1951\n",
            "Epoch 399/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.4788 - accuracy: 0.2875 - val_loss: 2.6412 - val_accuracy: 0.2326\n",
            "Epoch 400/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4793 - accuracy: 0.2854 - val_loss: 2.7186 - val_accuracy: 0.2571\n",
            "Epoch 401/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4881 - accuracy: 0.2860 - val_loss: 2.7646 - val_accuracy: 0.2318\n",
            "Epoch 402/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4635 - accuracy: 0.2894 - val_loss: 2.6034 - val_accuracy: 0.2767\n",
            "Epoch 403/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4889 - accuracy: 0.2845 - val_loss: 2.9906 - val_accuracy: 0.1938\n",
            "Epoch 404/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4833 - accuracy: 0.2898 - val_loss: 2.4476 - val_accuracy: 0.3147\n",
            "Epoch 405/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4660 - accuracy: 0.2888 - val_loss: 3.1267 - val_accuracy: 0.1759\n",
            "Epoch 406/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.4666 - accuracy: 0.2859 - val_loss: 3.1059 - val_accuracy: 0.2086\n",
            "Epoch 407/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4807 - accuracy: 0.2876 - val_loss: 2.8157 - val_accuracy: 0.2182\n",
            "Epoch 408/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4643 - accuracy: 0.2866 - val_loss: 3.1303 - val_accuracy: 0.1982\n",
            "Epoch 409/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4722 - accuracy: 0.2883 - val_loss: 2.9967 - val_accuracy: 0.2047\n",
            "Epoch 410/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4674 - accuracy: 0.2856 - val_loss: 3.6505 - val_accuracy: 0.1340\n",
            "Epoch 411/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4945 - accuracy: 0.2866 - val_loss: 2.5131 - val_accuracy: 0.2593\n",
            "Epoch 412/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4720 - accuracy: 0.2904 - val_loss: 3.5905 - val_accuracy: 0.2086\n",
            "Epoch 413/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4725 - accuracy: 0.2876 - val_loss: 2.7289 - val_accuracy: 0.2139\n",
            "Epoch 414/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4821 - accuracy: 0.2864 - val_loss: 2.8486 - val_accuracy: 0.2859\n",
            "Epoch 415/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4489 - accuracy: 0.2907 - val_loss: 2.8306 - val_accuracy: 0.2584\n",
            "Epoch 416/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.4770 - accuracy: 0.2886 - val_loss: 2.6639 - val_accuracy: 0.2562\n",
            "Epoch 417/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4452 - accuracy: 0.2885 - val_loss: 2.5962 - val_accuracy: 0.2724\n",
            "Epoch 418/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4596 - accuracy: 0.2896 - val_loss: 3.0564 - val_accuracy: 0.2331\n",
            "Epoch 419/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4349 - accuracy: 0.2926 - val_loss: 2.7233 - val_accuracy: 0.2929\n",
            "Epoch 420/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4497 - accuracy: 0.2895 - val_loss: 2.7093 - val_accuracy: 0.2567\n",
            "Epoch 421/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4564 - accuracy: 0.2925 - val_loss: 2.7267 - val_accuracy: 0.2532\n",
            "Epoch 422/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4514 - accuracy: 0.2890 - val_loss: 2.9223 - val_accuracy: 0.2719\n",
            "Epoch 423/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4531 - accuracy: 0.2897 - val_loss: 3.0792 - val_accuracy: 0.1925\n",
            "Epoch 424/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.4496 - accuracy: 0.2923 - val_loss: 3.3254 - val_accuracy: 0.2078\n",
            "Epoch 425/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4558 - accuracy: 0.2853 - val_loss: 3.1614 - val_accuracy: 0.2082\n",
            "Epoch 426/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4451 - accuracy: 0.2932 - val_loss: 2.8611 - val_accuracy: 0.2484\n",
            "Epoch 427/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4660 - accuracy: 0.2848 - val_loss: 2.8410 - val_accuracy: 0.2366\n",
            "Epoch 428/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4411 - accuracy: 0.2889 - val_loss: 2.8692 - val_accuracy: 0.2444\n",
            "Epoch 429/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4505 - accuracy: 0.2881 - val_loss: 3.1371 - val_accuracy: 0.1777\n",
            "Epoch 430/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4718 - accuracy: 0.2891 - val_loss: 2.7810 - val_accuracy: 0.2457\n",
            "Epoch 431/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4453 - accuracy: 0.2896 - val_loss: 2.9727 - val_accuracy: 0.2252\n",
            "Epoch 432/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4533 - accuracy: 0.2886 - val_loss: 2.9523 - val_accuracy: 0.1790\n",
            "Epoch 433/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4237 - accuracy: 0.2952 - val_loss: 3.5544 - val_accuracy: 0.1436\n",
            "Epoch 434/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.4374 - accuracy: 0.2904 - val_loss: 3.0833 - val_accuracy: 0.1641\n",
            "Epoch 435/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4506 - accuracy: 0.2897 - val_loss: 2.7064 - val_accuracy: 0.2226\n",
            "Epoch 436/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4417 - accuracy: 0.2850 - val_loss: 3.3601 - val_accuracy: 0.1257\n",
            "Epoch 437/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4239 - accuracy: 0.2921 - val_loss: 3.0448 - val_accuracy: 0.2283\n",
            "Epoch 438/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4319 - accuracy: 0.2888 - val_loss: 2.8178 - val_accuracy: 0.2196\n",
            "Epoch 439/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4386 - accuracy: 0.2890 - val_loss: 2.9406 - val_accuracy: 0.2654\n",
            "Epoch 440/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4326 - accuracy: 0.2901 - val_loss: 2.7504 - val_accuracy: 0.2772\n",
            "Epoch 441/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4429 - accuracy: 0.2917 - val_loss: 2.9722 - val_accuracy: 0.2091\n",
            "Epoch 442/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4302 - accuracy: 0.2897 - val_loss: 2.7358 - val_accuracy: 0.2558\n",
            "Epoch 443/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4531 - accuracy: 0.2883 - val_loss: 2.8118 - val_accuracy: 0.2134\n",
            "Epoch 444/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4301 - accuracy: 0.2899 - val_loss: 3.1123 - val_accuracy: 0.2309\n",
            "Epoch 445/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4436 - accuracy: 0.2885 - val_loss: 2.4936 - val_accuracy: 0.3038\n",
            "Epoch 446/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4305 - accuracy: 0.2900 - val_loss: 3.1294 - val_accuracy: 0.1606\n",
            "Epoch 447/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4298 - accuracy: 0.2914 - val_loss: 3.3033 - val_accuracy: 0.1921\n",
            "Epoch 448/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4308 - accuracy: 0.2920 - val_loss: 2.9105 - val_accuracy: 0.2130\n",
            "Epoch 449/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4302 - accuracy: 0.2901 - val_loss: 2.7354 - val_accuracy: 0.2968\n",
            "Epoch 450/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4313 - accuracy: 0.2915 - val_loss: 2.5565 - val_accuracy: 0.2471\n",
            "Epoch 451/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4219 - accuracy: 0.2931 - val_loss: 3.0909 - val_accuracy: 0.1912\n",
            "Epoch 452/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.4498 - accuracy: 0.2911 - val_loss: 2.6473 - val_accuracy: 0.2235\n",
            "Epoch 453/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4294 - accuracy: 0.2866 - val_loss: 3.5251 - val_accuracy: 0.1768\n",
            "Epoch 454/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4419 - accuracy: 0.2859 - val_loss: 2.5796 - val_accuracy: 0.2776\n",
            "Epoch 455/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4412 - accuracy: 0.2882 - val_loss: 3.4757 - val_accuracy: 0.1235\n",
            "Epoch 456/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4560 - accuracy: 0.2918 - val_loss: 2.5237 - val_accuracy: 0.3064\n",
            "Epoch 457/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4284 - accuracy: 0.2902 - val_loss: 2.4411 - val_accuracy: 0.2815\n",
            "Epoch 458/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4027 - accuracy: 0.2922 - val_loss: 2.6846 - val_accuracy: 0.2466\n",
            "Epoch 459/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4335 - accuracy: 0.2929 - val_loss: 2.9723 - val_accuracy: 0.1886\n",
            "Epoch 460/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4355 - accuracy: 0.2931 - val_loss: 3.5047 - val_accuracy: 0.2746\n",
            "Epoch 461/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4486 - accuracy: 0.2923 - val_loss: 3.2058 - val_accuracy: 0.1790\n",
            "Epoch 462/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4303 - accuracy: 0.2911 - val_loss: 3.5567 - val_accuracy: 0.1227\n",
            "Epoch 463/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4123 - accuracy: 0.2939 - val_loss: 2.6584 - val_accuracy: 0.2929\n",
            "Epoch 464/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4068 - accuracy: 0.2900 - val_loss: 3.1108 - val_accuracy: 0.1964\n",
            "Epoch 465/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4376 - accuracy: 0.2915 - val_loss: 3.0779 - val_accuracy: 0.1646\n",
            "Epoch 466/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4281 - accuracy: 0.2892 - val_loss: 3.0779 - val_accuracy: 0.2052\n",
            "Epoch 467/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4077 - accuracy: 0.2905 - val_loss: 2.5226 - val_accuracy: 0.2833\n",
            "Epoch 468/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.4562 - accuracy: 0.2912 - val_loss: 2.9246 - val_accuracy: 0.2737\n",
            "Epoch 469/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4202 - accuracy: 0.2936 - val_loss: 2.9622 - val_accuracy: 0.2388\n",
            "Epoch 470/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3876 - accuracy: 0.2926 - val_loss: 2.6639 - val_accuracy: 0.2466\n",
            "Epoch 471/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4214 - accuracy: 0.2912 - val_loss: 2.9198 - val_accuracy: 0.2741\n",
            "Epoch 472/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4221 - accuracy: 0.2925 - val_loss: 2.8831 - val_accuracy: 0.2383\n",
            "Epoch 473/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4247 - accuracy: 0.2950 - val_loss: 3.4939 - val_accuracy: 0.2728\n",
            "Epoch 474/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4239 - accuracy: 0.2933 - val_loss: 2.7228 - val_accuracy: 0.2348\n",
            "Epoch 475/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4024 - accuracy: 0.2919 - val_loss: 2.7454 - val_accuracy: 0.2401\n",
            "Epoch 476/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.4229 - accuracy: 0.2944 - val_loss: 3.2803 - val_accuracy: 0.2003\n",
            "Epoch 477/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4013 - accuracy: 0.2925 - val_loss: 2.5992 - val_accuracy: 0.2680\n",
            "Epoch 478/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4080 - accuracy: 0.2958 - val_loss: 2.8570 - val_accuracy: 0.2606\n",
            "Epoch 479/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3989 - accuracy: 0.2984 - val_loss: 3.0800 - val_accuracy: 0.2636\n",
            "Epoch 480/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4205 - accuracy: 0.2905 - val_loss: 2.7717 - val_accuracy: 0.1916\n",
            "Epoch 481/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4340 - accuracy: 0.2934 - val_loss: 3.7332 - val_accuracy: 0.1423\n",
            "Epoch 482/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4444 - accuracy: 0.2918 - val_loss: 3.4386 - val_accuracy: 0.1947\n",
            "Epoch 483/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3967 - accuracy: 0.2962 - val_loss: 2.9237 - val_accuracy: 0.2331\n",
            "Epoch 484/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4341 - accuracy: 0.2952 - val_loss: 3.0581 - val_accuracy: 0.1842\n",
            "Epoch 485/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4232 - accuracy: 0.2896 - val_loss: 2.8173 - val_accuracy: 0.2811\n",
            "Epoch 486/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3834 - accuracy: 0.2970 - val_loss: 2.8323 - val_accuracy: 0.2802\n",
            "Epoch 487/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4025 - accuracy: 0.2932 - val_loss: 3.0374 - val_accuracy: 0.1781\n",
            "Epoch 488/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4058 - accuracy: 0.2937 - val_loss: 2.5183 - val_accuracy: 0.3016\n",
            "Epoch 489/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4694 - accuracy: 0.2883 - val_loss: 3.0792 - val_accuracy: 0.1982\n",
            "Epoch 490/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3954 - accuracy: 0.2920 - val_loss: 3.2044 - val_accuracy: 0.2505\n",
            "Epoch 491/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4114 - accuracy: 0.2910 - val_loss: 2.6735 - val_accuracy: 0.2759\n",
            "Epoch 492/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4116 - accuracy: 0.2931 - val_loss: 3.0092 - val_accuracy: 0.2462\n",
            "Epoch 493/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.4156 - accuracy: 0.2939 - val_loss: 2.3997 - val_accuracy: 0.3409\n",
            "Epoch 494/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4078 - accuracy: 0.2921 - val_loss: 2.7809 - val_accuracy: 0.2484\n",
            "Epoch 495/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3888 - accuracy: 0.2977 - val_loss: 2.6875 - val_accuracy: 0.2667\n",
            "Epoch 496/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4030 - accuracy: 0.2961 - val_loss: 2.6412 - val_accuracy: 0.2959\n",
            "Epoch 497/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4353 - accuracy: 0.2869 - val_loss: 2.5418 - val_accuracy: 0.2741\n",
            "Epoch 498/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3965 - accuracy: 0.2924 - val_loss: 2.6505 - val_accuracy: 0.2973\n",
            "Epoch 499/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4135 - accuracy: 0.2933 - val_loss: 2.6694 - val_accuracy: 0.2510\n",
            "Epoch 500/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4133 - accuracy: 0.2900 - val_loss: 2.9593 - val_accuracy: 0.2680\n",
            "Epoch 501/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4046 - accuracy: 0.2964 - val_loss: 2.6687 - val_accuracy: 0.2623\n",
            "Epoch 502/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4229 - accuracy: 0.2909 - val_loss: 3.4884 - val_accuracy: 0.2104\n",
            "Epoch 503/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4000 - accuracy: 0.2965 - val_loss: 2.8047 - val_accuracy: 0.2043\n",
            "Epoch 504/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3871 - accuracy: 0.2949 - val_loss: 3.0212 - val_accuracy: 0.1934\n",
            "Epoch 505/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3961 - accuracy: 0.2937 - val_loss: 2.7778 - val_accuracy: 0.2340\n",
            "Epoch 506/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3901 - accuracy: 0.2963 - val_loss: 2.6455 - val_accuracy: 0.2689\n",
            "Epoch 507/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4201 - accuracy: 0.2881 - val_loss: 2.7170 - val_accuracy: 0.2436\n",
            "Epoch 508/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3922 - accuracy: 0.2927 - val_loss: 2.8888 - val_accuracy: 0.2370\n",
            "Epoch 509/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4029 - accuracy: 0.2931 - val_loss: 2.9680 - val_accuracy: 0.2601\n",
            "Epoch 510/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4089 - accuracy: 0.2875 - val_loss: 2.8034 - val_accuracy: 0.2501\n",
            "Epoch 511/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3950 - accuracy: 0.2927 - val_loss: 3.1822 - val_accuracy: 0.1336\n",
            "Epoch 512/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4054 - accuracy: 0.2917 - val_loss: 2.7174 - val_accuracy: 0.2449\n",
            "Epoch 513/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4188 - accuracy: 0.2928 - val_loss: 2.8493 - val_accuracy: 0.2196\n",
            "Epoch 514/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4045 - accuracy: 0.2928 - val_loss: 2.9142 - val_accuracy: 0.2292\n",
            "Epoch 515/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4018 - accuracy: 0.2916 - val_loss: 3.5676 - val_accuracy: 0.2396\n",
            "Epoch 516/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3880 - accuracy: 0.2942 - val_loss: 2.6737 - val_accuracy: 0.2492\n",
            "Epoch 517/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3940 - accuracy: 0.2957 - val_loss: 2.5216 - val_accuracy: 0.2732\n",
            "Epoch 518/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4223 - accuracy: 0.2885 - val_loss: 2.8596 - val_accuracy: 0.2003\n",
            "Epoch 519/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3869 - accuracy: 0.2883 - val_loss: 2.7476 - val_accuracy: 0.2654\n",
            "Epoch 520/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3761 - accuracy: 0.2953 - val_loss: 3.4844 - val_accuracy: 0.2654\n",
            "Epoch 521/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3866 - accuracy: 0.2955 - val_loss: 3.2510 - val_accuracy: 0.2436\n",
            "Epoch 522/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4195 - accuracy: 0.2947 - val_loss: 2.6918 - val_accuracy: 0.2746\n",
            "Epoch 523/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3997 - accuracy: 0.2962 - val_loss: 3.7140 - val_accuracy: 0.1654\n",
            "Epoch 524/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3998 - accuracy: 0.2904 - val_loss: 2.6986 - val_accuracy: 0.2689\n",
            "Epoch 525/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3991 - accuracy: 0.2948 - val_loss: 2.4926 - val_accuracy: 0.2811\n",
            "Epoch 526/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4014 - accuracy: 0.2898 - val_loss: 2.9642 - val_accuracy: 0.2056\n",
            "Epoch 527/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4082 - accuracy: 0.2927 - val_loss: 2.4424 - val_accuracy: 0.3108\n",
            "Epoch 528/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3788 - accuracy: 0.2965 - val_loss: 2.8188 - val_accuracy: 0.2409\n",
            "Epoch 529/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3716 - accuracy: 0.2973 - val_loss: 3.0452 - val_accuracy: 0.1851\n",
            "Epoch 530/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4056 - accuracy: 0.2944 - val_loss: 3.4121 - val_accuracy: 0.2034\n",
            "Epoch 531/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3785 - accuracy: 0.2976 - val_loss: 2.9362 - val_accuracy: 0.1702\n",
            "Epoch 532/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3897 - accuracy: 0.2938 - val_loss: 2.7491 - val_accuracy: 0.2270\n",
            "Epoch 533/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3998 - accuracy: 0.2902 - val_loss: 3.7443 - val_accuracy: 0.1576\n",
            "Epoch 534/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4057 - accuracy: 0.2908 - val_loss: 2.7376 - val_accuracy: 0.2549\n",
            "Epoch 535/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3911 - accuracy: 0.2927 - val_loss: 2.8644 - val_accuracy: 0.2436\n",
            "Epoch 536/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3991 - accuracy: 0.2926 - val_loss: 2.9141 - val_accuracy: 0.2562\n",
            "Epoch 537/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4229 - accuracy: 0.2953 - val_loss: 3.1511 - val_accuracy: 0.2798\n",
            "Epoch 538/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3861 - accuracy: 0.2967 - val_loss: 3.0216 - val_accuracy: 0.2558\n",
            "Epoch 539/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3869 - accuracy: 0.2925 - val_loss: 2.7137 - val_accuracy: 0.2693\n",
            "Epoch 540/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3964 - accuracy: 0.2935 - val_loss: 2.4020 - val_accuracy: 0.3038\n",
            "Epoch 541/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4064 - accuracy: 0.2929 - val_loss: 2.8942 - val_accuracy: 0.1755\n",
            "Epoch 542/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3815 - accuracy: 0.2960 - val_loss: 2.7324 - val_accuracy: 0.2370\n",
            "Epoch 543/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4096 - accuracy: 0.2935 - val_loss: 2.6695 - val_accuracy: 0.1934\n",
            "Epoch 544/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3937 - accuracy: 0.2920 - val_loss: 3.0104 - val_accuracy: 0.1947\n",
            "Epoch 545/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3946 - accuracy: 0.2943 - val_loss: 2.6168 - val_accuracy: 0.2545\n",
            "Epoch 546/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3843 - accuracy: 0.2977 - val_loss: 2.6383 - val_accuracy: 0.2344\n",
            "Epoch 547/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4046 - accuracy: 0.2906 - val_loss: 2.8908 - val_accuracy: 0.1995\n",
            "Epoch 548/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3700 - accuracy: 0.2980 - val_loss: 2.6987 - val_accuracy: 0.2257\n",
            "Epoch 549/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3742 - accuracy: 0.2972 - val_loss: 2.6917 - val_accuracy: 0.2702\n",
            "Epoch 550/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3932 - accuracy: 0.2964 - val_loss: 2.7284 - val_accuracy: 0.2540\n",
            "Epoch 551/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4296 - accuracy: 0.2894 - val_loss: 2.9813 - val_accuracy: 0.1890\n",
            "Epoch 552/1500\n",
            "453/453 [==============================] - 1s 2ms/step - loss: 2.3837 - accuracy: 0.2958 - val_loss: 2.8314 - val_accuracy: 0.2536\n",
            "Epoch 553/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3784 - accuracy: 0.2942 - val_loss: 2.8764 - val_accuracy: 0.1938\n",
            "Epoch 554/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3879 - accuracy: 0.2942 - val_loss: 2.8392 - val_accuracy: 0.2121\n",
            "Epoch 555/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3788 - accuracy: 0.2910 - val_loss: 2.9185 - val_accuracy: 0.1977\n",
            "Epoch 556/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4012 - accuracy: 0.2929 - val_loss: 2.7273 - val_accuracy: 0.2283\n",
            "Epoch 557/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3680 - accuracy: 0.2942 - val_loss: 2.7121 - val_accuracy: 0.2479\n",
            "Epoch 558/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4058 - accuracy: 0.2956 - val_loss: 2.6680 - val_accuracy: 0.2536\n",
            "Epoch 559/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3757 - accuracy: 0.2954 - val_loss: 3.2438 - val_accuracy: 0.1611\n",
            "Epoch 560/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3931 - accuracy: 0.2887 - val_loss: 2.7872 - val_accuracy: 0.2698\n",
            "Epoch 561/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3994 - accuracy: 0.2931 - val_loss: 3.2018 - val_accuracy: 0.2187\n",
            "Epoch 562/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3688 - accuracy: 0.2971 - val_loss: 2.9476 - val_accuracy: 0.2169\n",
            "Epoch 563/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3928 - accuracy: 0.2937 - val_loss: 3.2238 - val_accuracy: 0.2711\n",
            "Epoch 564/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3751 - accuracy: 0.2968 - val_loss: 2.7427 - val_accuracy: 0.2409\n",
            "Epoch 565/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3859 - accuracy: 0.2911 - val_loss: 2.5798 - val_accuracy: 0.2514\n",
            "Epoch 566/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4147 - accuracy: 0.2921 - val_loss: 2.7417 - val_accuracy: 0.2667\n",
            "Epoch 567/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3678 - accuracy: 0.2945 - val_loss: 3.0448 - val_accuracy: 0.2532\n",
            "Epoch 568/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3430 - accuracy: 0.2972 - val_loss: 2.9987 - val_accuracy: 0.1257\n",
            "Epoch 569/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3705 - accuracy: 0.2962 - val_loss: 2.6186 - val_accuracy: 0.2859\n",
            "Epoch 570/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3738 - accuracy: 0.2977 - val_loss: 2.7999 - val_accuracy: 0.2318\n",
            "Epoch 571/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3943 - accuracy: 0.2905 - val_loss: 2.8536 - val_accuracy: 0.2038\n",
            "Epoch 572/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3821 - accuracy: 0.2953 - val_loss: 2.7568 - val_accuracy: 0.2148\n",
            "Epoch 573/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3754 - accuracy: 0.2968 - val_loss: 2.8316 - val_accuracy: 0.2715\n",
            "Epoch 574/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3885 - accuracy: 0.2947 - val_loss: 3.0013 - val_accuracy: 0.2017\n",
            "Epoch 575/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3653 - accuracy: 0.2972 - val_loss: 3.0023 - val_accuracy: 0.2126\n",
            "Epoch 576/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3781 - accuracy: 0.2939 - val_loss: 2.9739 - val_accuracy: 0.2326\n",
            "Epoch 577/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3917 - accuracy: 0.2911 - val_loss: 3.8666 - val_accuracy: 0.1632\n",
            "Epoch 578/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3692 - accuracy: 0.2967 - val_loss: 2.5835 - val_accuracy: 0.3003\n",
            "Epoch 579/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4213 - accuracy: 0.2950 - val_loss: 2.7516 - val_accuracy: 0.2794\n",
            "Epoch 580/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3864 - accuracy: 0.2963 - val_loss: 3.0548 - val_accuracy: 0.2139\n",
            "Epoch 581/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3705 - accuracy: 0.2956 - val_loss: 2.4864 - val_accuracy: 0.2850\n",
            "Epoch 582/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3860 - accuracy: 0.2953 - val_loss: 3.3483 - val_accuracy: 0.2226\n",
            "Epoch 583/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3878 - accuracy: 0.2985 - val_loss: 2.6732 - val_accuracy: 0.2287\n",
            "Epoch 584/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3587 - accuracy: 0.2950 - val_loss: 2.9216 - val_accuracy: 0.2741\n",
            "Epoch 585/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3753 - accuracy: 0.2961 - val_loss: 3.8167 - val_accuracy: 0.1323\n",
            "Epoch 586/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3810 - accuracy: 0.2941 - val_loss: 2.6297 - val_accuracy: 0.2340\n",
            "Epoch 587/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3726 - accuracy: 0.2975 - val_loss: 2.9537 - val_accuracy: 0.2292\n",
            "Epoch 588/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3898 - accuracy: 0.2903 - val_loss: 2.6788 - val_accuracy: 0.2108\n",
            "Epoch 589/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3914 - accuracy: 0.2964 - val_loss: 3.1445 - val_accuracy: 0.1589\n",
            "Epoch 590/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3620 - accuracy: 0.2975 - val_loss: 2.6798 - val_accuracy: 0.2038\n",
            "Epoch 591/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3806 - accuracy: 0.2982 - val_loss: 3.1245 - val_accuracy: 0.1916\n",
            "Epoch 592/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3850 - accuracy: 0.2924 - val_loss: 2.7779 - val_accuracy: 0.2680\n",
            "Epoch 593/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3948 - accuracy: 0.2934 - val_loss: 2.8116 - val_accuracy: 0.2283\n",
            "Epoch 594/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3715 - accuracy: 0.2954 - val_loss: 2.8329 - val_accuracy: 0.2161\n",
            "Epoch 595/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3768 - accuracy: 0.2903 - val_loss: 2.6970 - val_accuracy: 0.2623\n",
            "Epoch 596/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3816 - accuracy: 0.2970 - val_loss: 2.4591 - val_accuracy: 0.2828\n",
            "Epoch 597/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3722 - accuracy: 0.2945 - val_loss: 2.8782 - val_accuracy: 0.2754\n",
            "Epoch 598/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3751 - accuracy: 0.2957 - val_loss: 2.5098 - val_accuracy: 0.2628\n",
            "Epoch 599/1500\n",
            "453/453 [==============================] - 2s 3ms/step - loss: 2.3904 - accuracy: 0.2990 - val_loss: 2.8059 - val_accuracy: 0.2536\n",
            "Epoch 600/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3665 - accuracy: 0.2952 - val_loss: 2.5390 - val_accuracy: 0.2698\n",
            "Epoch 601/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3838 - accuracy: 0.2928 - val_loss: 2.9745 - val_accuracy: 0.2069\n",
            "Epoch 602/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3594 - accuracy: 0.2965 - val_loss: 2.4833 - val_accuracy: 0.3038\n",
            "Epoch 603/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3888 - accuracy: 0.2973 - val_loss: 3.0276 - val_accuracy: 0.2575\n",
            "Epoch 604/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3748 - accuracy: 0.2962 - val_loss: 2.5287 - val_accuracy: 0.2645\n",
            "Epoch 605/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3634 - accuracy: 0.2958 - val_loss: 3.3401 - val_accuracy: 0.1838\n",
            "Epoch 606/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3923 - accuracy: 0.2935 - val_loss: 2.6705 - val_accuracy: 0.2798\n",
            "Epoch 607/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3747 - accuracy: 0.2994 - val_loss: 3.1737 - val_accuracy: 0.2514\n",
            "Epoch 608/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3622 - accuracy: 0.2997 - val_loss: 2.6528 - val_accuracy: 0.2191\n",
            "Epoch 609/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3723 - accuracy: 0.2982 - val_loss: 3.0397 - val_accuracy: 0.2187\n",
            "Epoch 610/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3724 - accuracy: 0.2980 - val_loss: 3.5983 - val_accuracy: 0.1043\n",
            "Epoch 611/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3960 - accuracy: 0.2993 - val_loss: 3.6261 - val_accuracy: 0.1654\n",
            "Epoch 612/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3870 - accuracy: 0.2960 - val_loss: 2.6520 - val_accuracy: 0.2876\n",
            "Epoch 613/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3724 - accuracy: 0.2944 - val_loss: 2.4845 - val_accuracy: 0.3007\n",
            "Epoch 614/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3787 - accuracy: 0.2932 - val_loss: 3.3624 - val_accuracy: 0.1694\n",
            "Epoch 615/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3537 - accuracy: 0.3003 - val_loss: 2.8508 - val_accuracy: 0.2505\n",
            "Epoch 616/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3622 - accuracy: 0.2919 - val_loss: 2.5237 - val_accuracy: 0.2641\n",
            "Epoch 617/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3911 - accuracy: 0.2908 - val_loss: 3.1371 - val_accuracy: 0.2785\n",
            "Epoch 618/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3649 - accuracy: 0.3006 - val_loss: 2.8014 - val_accuracy: 0.1842\n",
            "Epoch 619/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3851 - accuracy: 0.2947 - val_loss: 2.6983 - val_accuracy: 0.2383\n",
            "Epoch 620/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3510 - accuracy: 0.2993 - val_loss: 3.0570 - val_accuracy: 0.2562\n",
            "Epoch 621/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3685 - accuracy: 0.2985 - val_loss: 3.1001 - val_accuracy: 0.2305\n",
            "Epoch 622/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3670 - accuracy: 0.2978 - val_loss: 3.3118 - val_accuracy: 0.1593\n",
            "Epoch 623/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3846 - accuracy: 0.2939 - val_loss: 2.5582 - val_accuracy: 0.2872\n",
            "Epoch 624/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3486 - accuracy: 0.2983 - val_loss: 2.7808 - val_accuracy: 0.2213\n",
            "Epoch 625/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3704 - accuracy: 0.2956 - val_loss: 2.6366 - val_accuracy: 0.2785\n",
            "Epoch 626/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3746 - accuracy: 0.2976 - val_loss: 2.7494 - val_accuracy: 0.2846\n",
            "Epoch 627/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3527 - accuracy: 0.2980 - val_loss: 2.5975 - val_accuracy: 0.2654\n",
            "Epoch 628/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3812 - accuracy: 0.2947 - val_loss: 3.4302 - val_accuracy: 0.1484\n",
            "Epoch 629/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3580 - accuracy: 0.3009 - val_loss: 2.5373 - val_accuracy: 0.2226\n",
            "Epoch 630/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3756 - accuracy: 0.2943 - val_loss: 2.6808 - val_accuracy: 0.2505\n",
            "Epoch 631/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3858 - accuracy: 0.2977 - val_loss: 2.3324 - val_accuracy: 0.3134\n",
            "Epoch 632/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3729 - accuracy: 0.2968 - val_loss: 3.0986 - val_accuracy: 0.2226\n",
            "Epoch 633/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3613 - accuracy: 0.2970 - val_loss: 2.7721 - val_accuracy: 0.3095\n",
            "Epoch 634/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3903 - accuracy: 0.2931 - val_loss: 4.0116 - val_accuracy: 0.1528\n",
            "Epoch 635/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3528 - accuracy: 0.2980 - val_loss: 2.4915 - val_accuracy: 0.2593\n",
            "Epoch 636/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3672 - accuracy: 0.2989 - val_loss: 2.8356 - val_accuracy: 0.2658\n",
            "Epoch 637/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3740 - accuracy: 0.2983 - val_loss: 3.2444 - val_accuracy: 0.1388\n",
            "Epoch 638/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3666 - accuracy: 0.3001 - val_loss: 3.2871 - val_accuracy: 0.1641\n",
            "Epoch 639/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.4035 - accuracy: 0.2957 - val_loss: 3.2433 - val_accuracy: 0.1659\n",
            "Epoch 640/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3645 - accuracy: 0.2945 - val_loss: 2.5509 - val_accuracy: 0.2396\n",
            "Epoch 641/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3595 - accuracy: 0.2963 - val_loss: 2.8321 - val_accuracy: 0.1894\n",
            "Epoch 642/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3423 - accuracy: 0.2954 - val_loss: 2.7169 - val_accuracy: 0.2649\n",
            "Epoch 643/1500\n",
            "453/453 [==============================] - 1s 3ms/step - loss: 2.3579 - accuracy: 0.2978 - val_loss: 2.8713 - val_accuracy: 0.2558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_validation, np.argmax(model_NN.predict(X_validation), axis = 1))"
      ],
      "metadata": {
        "id": "7rAbl_V2yDew",
        "outputId": "3c30df36-33f1-45d2-f615-72dbd55978dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72/72 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.34089917066783065"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_k_accuracy_score(y_validation, model_NN.predict(X_validation), labels = np.arange(0,21,1))"
      ],
      "metadata": {
        "id": "HtAqZ2Z4-Ua_",
        "outputId": "dfa335da-128c-4f94-97c1-209e6da7c813",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72/72 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.509384548232213"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    }
  ]
}