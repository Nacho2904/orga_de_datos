{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4AipTu2A5dlGl5iKuMKF/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nacho2904/orga_de_datos/blob/main/tp3_parte_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-7fkwozC3D3",
        "outputId": "0d1c7d24-3214-430d-bed8-1b4f7c7f6d3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import top_k_accuracy_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from google.colab import drive \n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "import functools\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "path_a_training_set = 'gdrive/MyDrive/TP3 dataset music/train.parquet'\n",
        "path_a_test_set = 'gdrive/MyDrive/TP3 dataset music/test.parquet'\n",
        "\n",
        "\n",
        "df_music_train = pd.read_parquet(path_a_training_set).fillna(\"\")\n",
        "df_music_test = pd.read_parquet(path_a_test_set).fillna(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "fwptY_P6y2pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Todo el proceso estÃ¡ explicado en la parte II"
      ],
      "metadata": {
        "id": "CNQZRiXJC87d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ordinalEncoder = preprocessing.OrdinalEncoder(categories = [[\"Minor\", \"Major\"],['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B'],\n",
        "                                                            ['1/4', '3/4', '4/4', '5/4']])"
      ],
      "metadata": {
        "id": "dQohiJ4Lhn86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('vader_lexicon')\n",
        "def apply_sentiment_analysis_to_lyrics(df_music: pd.DataFrame) -> pd.DataFrame:\n",
        "  sia = SentimentIntensityAnalyzer()\n",
        "  negative, neutral, positive, compound = 0, 1, 2, 3\n",
        "  sentimentAnalysisOfLyrics = df_music[\"lyric\"].map(lambda lyric: list(sia.polarity_scores(lyric).values()))\n",
        "  negativeScoreOfLyrics = sentimentAnalysisOfLyrics.map(lambda row: row[negative])\n",
        "  positiveScoreOfLyrics = sentimentAnalysisOfLyrics.map(lambda row: row[positive])\n",
        "  neutralScoreOfLyrics = sentimentAnalysisOfLyrics.map(lambda row: row[neutral])\n",
        "  compoundScoreOfLyrics = sentimentAnalysisOfLyrics.map(lambda row: row[compound])\n",
        "  return pd.DataFrame(pd.concat([negativeScoreOfLyrics, positiveScoreOfLyrics,neutralScoreOfLyrics,compoundScoreOfLyrics], axis = 1))"
      ],
      "metadata": {
        "id": "hMMWFU6D4BON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22cabd2a-28ea-4aea-b1aa-b367a5ebe694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "def get_sum_tfidf_from_lyrics(df_music: pd.DataFrame) -> pd.DataFrame:\n",
        "  stopwords = set(nltk.corpus.stopwords.words(\"english\")).union(set(nltk.corpus.stopwords.words(\"spanish\"))).union(set(nltk.corpus.stopwords.words(\"french\")))\n",
        "  vectorizer = TfidfVectorizer(input = \"content\", stop_words = stopwords)\n",
        "  vectorizer.fit(df_music[\"lyric\"])\n",
        "  return pd.DataFrame([np.sum(tfidf_vector) for tfidf_vector in vectorizer.transform(df_music[\"lyric\"])])"
      ],
      "metadata": {
        "id": "seazRU21KuOZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d6c1bf-f468-411a-c11a-5b59204b8d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eliminate_genres_without_enough_observations(df_music: pd.DataFrame) -> pd.DataFrame:\n",
        "  df_music_recuento_filas_por_genero = df_music.groupby(\"genre\").count().reset_index()[[\"genre\", \"track_name\"]].rename(\n",
        "    columns = {\"track_name\": \"rowCount\"}).sort_values(\"rowCount\")\n",
        "  problematic_genres = list(df_music_recuento_filas_por_genero[df_music_recuento_filas_por_genero[\"rowCount\"] < 50].genre)[1:]\n",
        "  return df_music[~df_music[\"genre\"].isin(problematic_genres)]"
      ],
      "metadata": {
        "id": "TQCnc16lQAxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "def get_length_transforms_for_text(df_music: pd.DataFrame) -> pd.DataFrame:\n",
        "  df_music[\"number_of_lines\"] = df_music[\"lyric\"].map(lambda lyric: len(lyric.split(\"\\n\")))\n",
        "  df_music[\"number_of_tokens\"] = df_music[\"lyric\"].map(lambda lyric: len(nltk.word_tokenize(lyric)))\n",
        "  df_music[\"length_lyrics\"] = df_music[\"lyric\"].map(lambda lyric: len(lyric))\n",
        "  df_music[\"length_of_track_name\"] = df_music[\"track_name\"].map(lambda track_name: len(track_name))\n",
        "  df_music[\"tokens_of_track_name\"] = df_music[\"track_name\"].map(lambda track_name: len(nltk.word_tokenize(track_name)))\n",
        "  return df_music[[\"length_lyrics\", \"length_of_track_name\", \"number_of_lines\", \"number_of_tokens\", \"tokens_of_track_name\"]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1QqQUHIEHzj",
        "outputId": "e8f0e51d-e9b6-4982-f7ce-37d2024692a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.pos_tag(nltk.word_tokenize(\"Hey how are you\"))\n",
        "useful_pos_tags = {\"FW\", \"JJR\", \"JJS\", \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"PRP\", \"RB\", \"RBR\", \"RBS\",\n",
        "                   \"UH\", \"VB\", \"VBG\", \"VBD\", \"VBN\", \"VBP\", \"VBZ\"}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeUMKy6VRbbn",
        "outputId": "69366d03-6c8f-4d38-f443-907f094203b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_POS_vector_from_lyric_POS(lyric_POS):\n",
        "  useful_pos_tags = [\"FW\", \"JJR\", \"JJS\", \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"PRP\", \"RB\", \"RBR\", \"RBS\",\n",
        "                   \"UH\", \"VB\", \"VBG\", \"VBD\", \"VBN\", \"VBP\", \"VBZ\"]\n",
        "  POS_dictionary = {pos_tag:0 for pos_tag in useful_pos_tags}\n",
        "  POS_dictionary.update({\"other\":0})\n",
        "\n",
        "  for token in lyric_POS:\n",
        "    if token[1] in useful_pos_tags:\n",
        "      POS_dictionary[token[1]] += 1\n",
        "    else:\n",
        "      POS_dictionary[\"other\"] += 1\n",
        "\n",
        "  return [POS_dictionary[pos_tag] for pos_tag in POS_dictionary]\n",
        "  \n",
        "\n",
        "def get_POS_chunk_taggin_counts_for_text(df_music: pd.DataFrame) -> pd.DataFrame:\n",
        "  df_music[\"lyric_POS\"] = df_music[\"lyric\"].map(lambda lyric: nltk.pos_tag(nltk.word_tokenize(lyric)))\n",
        "  df_music[\"lyric_POS\"] = df_music[\"lyric_POS\"].map(lambda lyric_pos: get_POS_vector_from_lyric_POS(lyric_pos))\n",
        "  return pd.DataFrame(df_music[\"lyric_POS\"].to_list())\n"
      ],
      "metadata": {
        "id": "LOEyM2E-Ny6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_music_filtered_genres = eliminate_genres_without_enough_observations(df_music_train_filtered)\n",
        "artists = list(df_music_filtered_genres[\"artist\"].unique())\n",
        "train_artists = set(artists[:int(0.8*len(artists))])\n",
        "validation_artists = set(artists[int(0.8*len(artists)):])\n",
        "train_set = df_music_filtered_genres[df_music_filtered_genres[\"artist\"].isin(train_artists)]\n",
        "validation_set = df_music_filtered_genres[df_music_filtered_genres[\"artist\"].isin(validation_artists)]"
      ],
      "metadata": {
        "id": "0WXfDLhA174X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "text_features = [\"track_name\", \"lyric\", \"artist\"]\n",
        "\n",
        "numerical_features = [\"a_songs\", \"a_popularity\", \"popularity\", \"acousticness\", \"danceability\", \"duration_ms\",\n",
        "                   \"energy\", \"instrumentalness\", \"liveness\", \"loudness\", \"speechiness\", \"tempo\", \"valence\"]\n",
        "\n",
        "ordinal_features = [\"mode\", \"key\", \"time_signature\"]\n",
        "\n",
        "one_hot_features = [\"language\"]\n",
        "\n",
        "artist_genres = [\"a_genres\", \"genre\"]\n",
        "\n",
        "label = [\"genre\"]\n",
        "\n",
        "identity_transformer = preprocessing.FunctionTransformer(None)\n",
        "\n",
        "full_processor = ColumnTransformer(transformers=[\n",
        "    ('text_sentiment_analysis', preprocessing.FunctionTransformer(apply_sentiment_analysis_to_lyrics), text_features),\n",
        "    ('text_tf_idf', preprocessing.FunctionTransformer(get_sum_tfidf_from_lyrics), text_features),\n",
        "    ('text_simple_transforms', preprocessing.FunctionTransformer(get_length_transforms_for_text), text_features),\n",
        "    ('text_POS_count', preprocessing.FunctionTransformer(get_POS_chunk_taggin_counts_for_text), text_features),\n",
        "    ('one-hot', preprocessing.OneHotEncoder(), one_hot_features),\n",
        "    ('numerical', identity_transformer, numerical_features),\n",
        "    ('ordinal', ordinalEncoder, ordinal_features),\n",
        "])\n",
        "\n",
        "logistic_regression_pipeline = Pipeline(steps = [\n",
        "    ('preprocess_X', full_processor),\n",
        "    ('model', LogisticRegression(penalty='l2', C = 1, solver = \"liblinear\", max_iter = 500, fit_intercept = True))\n",
        "])\n"
      ],
      "metadata": {
        "id": "ApBgPVOuocwU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}